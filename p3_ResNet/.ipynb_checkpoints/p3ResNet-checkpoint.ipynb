{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b2a7e-7a28-4df6-989f-b2df7cb63fe6",
   "metadata": {},
   "source": [
    "# Redes de Neuronas con conexiones residuales y entrenados según _transfer learning_\n",
    "## Práctica 3\n",
    "\n",
    "#### Hugo Fole Avellás y José Romero Conde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a49d-451a-4f55-bbc7-4337319a9cf0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782e9-0a87-4281-96f2-d5157ae3acae",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 1$  \n",
    " ### (_2 puntos_) Define la capa ResidualBlock (ver Figura 2), usando como base la plantilla proporcionada en la Figura 3.\n",
    " - Ten en cuenta que el número de convoluciones depende de los valores de input_channels y out-put_channels.\n",
    " - Esta red no tiene capas de Pooling. La reducción del tamaño se realiza con el parámetro strides\n",
    "de las convoluciones, pero se modifica únicamente en 1 (o 2) de las convoluciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04184d0-0eaf-4b05-b7b8-705e21ca01bd",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4d6140-6e4b-4812-9c5b-6b69730b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, input_channels, output_channels, strides=(1, 1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.BN1 = layers.BatchNormalization()\n",
    "        self.Conv1 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = strides,\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "                                   \n",
    "        self.BN2 = layers.BatchNormalization()\n",
    "        self.Conv2 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = (1, 1),\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "        \n",
    "        if input_channels != output_channels:\n",
    "            self.salidaDistinta = True\n",
    "            self.ConvFuera = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (1, 1),\n",
    "                                   strides = strides,\n",
    "                                   use_bias=False)\n",
    "        else: self.salidaDistinta = False\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.BN1(x)\n",
    "        y = activations.silu(x)\n",
    "        x = self.Conv1(y)\n",
    "        x = self.BN2(x)\n",
    "        x = activations.silu(x)\n",
    "        x = self.Conv2(x)\n",
    "        if self.salidaDistinta:\n",
    "            y = self.ConvFuera(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138fe8-eb0f-4fbf-b55e-58246474cd34",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 2$  \n",
    "### (_2 puntos_) Define la red ResidualNetwork (ver Figura 1). Para comprobar su correcto funcionamiento haz lo siguiente:\n",
    " - Descarga los pesos del modelo preentrenado (los podrás encontrar en el canal de Teams de la asignatura).\n",
    " - Carga los pesos en tu modelo, haciendo uso de la función proporcionada en la Figura 4.\n",
    " - Comprueba que la precisión del modelo en CIFAR-100 es superior al 69 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c77615-3123-4342-af5c-75949f51bf14",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e2d100-07f8-4029-a634-0063bc4a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "ResidualNetwork = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(100,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22577e69-b36b-407c-ad07-fc4949ca0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_weights(model, weight_file):\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    all_vars = model.trainable_weights + model.non_trainable_weights\n",
    "    weight_list = [(x, weights[x]) for x in sorted(weights.keys())]\n",
    "    weights = {}\n",
    "    for i, var in enumerate(all_vars):\n",
    "        aux = var.path.split('/')[-2:]\n",
    "        classname = '_'.join(aux[0].split('_')[:-1])\n",
    "        name = aux[1]\n",
    "        assigned = False\n",
    "        for j, (key, value) in enumerate(weight_list):\n",
    "            if classname in key and name in key:\n",
    "                try:\n",
    "                    all_vars[i].assign(value)\n",
    "                    print(':) ',end='')\n",
    "                except:\n",
    "                    continue\n",
    "                print('assinging', key, 'to', var.path)\n",
    "                del weight_list[j]\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            #raise Exception(var.path + ' cannot be loaded')\n",
    "            print(var.path + ' cannot be loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84ade66-b928-42be-bd8a-469e7bda95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) assinging conv2d_52/kernel to sequential_1/conv2d_22/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential_1/residual_block_9/batch_normalization_19/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential_1/residual_block_9/batch_normalization_19/beta\n",
      ":) assinging conv2d_54/kernel to sequential_1/residual_block_9/conv2d_23/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential_1/residual_block_9/batch_normalization_20/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential_1/residual_block_9/batch_normalization_20/beta\n",
      ":) assinging conv2d_55/kernel to sequential_1/residual_block_9/conv2d_24/kernel\n",
      ":) assinging conv2d_53/kernel to sequential_1/residual_block_9/conv2d_25/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential_1/residual_block_10/batch_normalization_21/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential_1/residual_block_10/batch_normalization_21/beta\n",
      ":) assinging conv2d_56/kernel to sequential_1/residual_block_10/conv2d_26/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential_1/residual_block_10/batch_normalization_22/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential_1/residual_block_10/batch_normalization_22/beta\n",
      ":) assinging conv2d_57/kernel to sequential_1/residual_block_10/conv2d_27/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential_1/residual_block_11/batch_normalization_23/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential_1/residual_block_11/batch_normalization_23/beta\n",
      ":) assinging conv2d_58/kernel to sequential_1/residual_block_11/conv2d_28/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential_1/residual_block_11/batch_normalization_24/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential_1/residual_block_11/batch_normalization_24/beta\n",
      ":) assinging conv2d_59/kernel to sequential_1/residual_block_11/conv2d_29/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential_1/residual_block_12/batch_normalization_25/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential_1/residual_block_12/batch_normalization_25/beta\n",
      ":) assinging conv2d_61/kernel to sequential_1/residual_block_12/conv2d_30/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential_1/residual_block_12/batch_normalization_26/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential_1/residual_block_12/batch_normalization_26/beta\n",
      ":) assinging conv2d_62/kernel to sequential_1/residual_block_12/conv2d_31/kernel\n",
      ":) assinging conv2d_60/kernel to sequential_1/residual_block_12/conv2d_32/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential_1/residual_block_13/batch_normalization_27/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential_1/residual_block_13/batch_normalization_27/beta\n",
      ":) assinging conv2d_63/kernel to sequential_1/residual_block_13/conv2d_33/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential_1/residual_block_13/batch_normalization_28/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential_1/residual_block_13/batch_normalization_28/beta\n",
      ":) assinging conv2d_64/kernel to sequential_1/residual_block_13/conv2d_34/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential_1/residual_block_14/batch_normalization_29/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential_1/residual_block_14/batch_normalization_29/beta\n",
      ":) assinging conv2d_65/kernel to sequential_1/residual_block_14/conv2d_35/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential_1/residual_block_14/batch_normalization_30/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential_1/residual_block_14/batch_normalization_30/beta\n",
      ":) assinging conv2d_66/kernel to sequential_1/residual_block_14/conv2d_36/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential_1/residual_block_15/batch_normalization_31/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential_1/residual_block_15/batch_normalization_31/beta\n",
      ":) assinging conv2d_68/kernel to sequential_1/residual_block_15/conv2d_37/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential_1/residual_block_15/batch_normalization_32/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential_1/residual_block_15/batch_normalization_32/beta\n",
      ":) assinging conv2d_69/kernel to sequential_1/residual_block_15/conv2d_38/kernel\n",
      ":) assinging conv2d_67/kernel to sequential_1/residual_block_15/conv2d_39/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential_1/residual_block_16/batch_normalization_33/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential_1/residual_block_16/batch_normalization_33/beta\n",
      ":) assinging conv2d_70/kernel to sequential_1/residual_block_16/conv2d_40/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential_1/residual_block_16/batch_normalization_34/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential_1/residual_block_16/batch_normalization_34/beta\n",
      ":) assinging conv2d_71/kernel to sequential_1/residual_block_16/conv2d_41/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential_1/residual_block_17/batch_normalization_35/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential_1/residual_block_17/batch_normalization_35/beta\n",
      ":) assinging conv2d_72/kernel to sequential_1/residual_block_17/conv2d_42/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential_1/residual_block_17/batch_normalization_36/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential_1/residual_block_17/batch_normalization_36/beta\n",
      ":) assinging conv2d_73/kernel to sequential_1/residual_block_17/conv2d_43/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential_1/batch_normalization_37/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential_1/batch_normalization_37/beta\n",
      ":) assinging dense_2/kernel to sequential_1/dense_1/kernel\n",
      ":) assinging dense_2/bias to sequential_1/dense_1/bias\n",
      ":) assinging batch_normalization_38/moving_mean to sequential_1/residual_block_9/batch_normalization_19/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential_1/residual_block_9/batch_normalization_19/moving_variance\n",
      ":) assinging batch_normalization_39/moving_mean to sequential_1/residual_block_9/batch_normalization_20/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential_1/residual_block_9/batch_normalization_20/moving_variance\n",
      ":) assinging batch_normalization_40/moving_mean to sequential_1/residual_block_10/batch_normalization_21/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential_1/residual_block_10/batch_normalization_21/moving_variance\n",
      ":) assinging batch_normalization_41/moving_mean to sequential_1/residual_block_10/batch_normalization_22/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential_1/residual_block_10/batch_normalization_22/moving_variance\n",
      ":) assinging batch_normalization_42/moving_mean to sequential_1/residual_block_11/batch_normalization_23/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential_1/residual_block_11/batch_normalization_23/moving_variance\n",
      ":) assinging batch_normalization_43/moving_mean to sequential_1/residual_block_11/batch_normalization_24/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential_1/residual_block_11/batch_normalization_24/moving_variance\n",
      ":) assinging batch_normalization_44/moving_mean to sequential_1/residual_block_12/batch_normalization_25/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential_1/residual_block_12/batch_normalization_25/moving_variance\n",
      ":) assinging batch_normalization_45/moving_mean to sequential_1/residual_block_12/batch_normalization_26/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential_1/residual_block_12/batch_normalization_26/moving_variance\n",
      ":) assinging batch_normalization_46/moving_mean to sequential_1/residual_block_13/batch_normalization_27/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential_1/residual_block_13/batch_normalization_27/moving_variance\n",
      ":) assinging batch_normalization_47/moving_mean to sequential_1/residual_block_13/batch_normalization_28/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential_1/residual_block_13/batch_normalization_28/moving_variance\n",
      ":) assinging batch_normalization_48/moving_mean to sequential_1/residual_block_14/batch_normalization_29/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential_1/residual_block_14/batch_normalization_29/moving_variance\n",
      ":) assinging batch_normalization_49/moving_mean to sequential_1/residual_block_14/batch_normalization_30/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential_1/residual_block_14/batch_normalization_30/moving_variance\n",
      ":) assinging batch_normalization_50/moving_mean to sequential_1/residual_block_15/batch_normalization_31/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential_1/residual_block_15/batch_normalization_31/moving_variance\n",
      ":) assinging batch_normalization_51/moving_mean to sequential_1/residual_block_15/batch_normalization_32/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential_1/residual_block_15/batch_normalization_32/moving_variance\n",
      ":) assinging batch_normalization_52/moving_mean to sequential_1/residual_block_16/batch_normalization_33/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential_1/residual_block_16/batch_normalization_33/moving_variance\n",
      ":) assinging batch_normalization_53/moving_mean to sequential_1/residual_block_16/batch_normalization_34/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential_1/residual_block_16/batch_normalization_34/moving_variance\n",
      ":) assinging batch_normalization_54/moving_mean to sequential_1/residual_block_17/batch_normalization_35/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential_1/residual_block_17/batch_normalization_35/moving_variance\n",
      ":) assinging batch_normalization_55/moving_mean to sequential_1/residual_block_17/batch_normalization_36/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential_1/residual_block_17/batch_normalization_36/moving_variance\n",
      ":) assinging batch_normalization_56/moving_mean to sequential_1/batch_normalization_37/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential_1/batch_normalization_37/moving_variance\n"
     ]
    }
   ],
   "source": [
    "load_weights(ResidualNetwork, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c97ae3-b928-4cd7-b7b8-e014b768726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## carga y procesado de datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919a7c33-d383-44da-b425-c6ab5b27d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.losses import CategoricalCrossentropy as CCE\n",
    "\n",
    "ResidualNetwork.compile(optimizer='adam',\n",
    "                        loss=CCE(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "ResidualNetwork.fit(x_train, y_train, \n",
    "                    epochs=5, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    batch_size=8)\n",
    "\n",
    "ResidualNetwork.evaluate(x_test)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ec35da2-f31a-4a58-aaec-01eaadeee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobación de que la carga de pesos fue correcta\n",
    "from random import randint\n",
    "\n",
    "def compruebaConTest(modelo, numeroEjemplos):\n",
    "    indices = [randint(0,x_test.shape[0]-1) for _ in range(numeroEjemplos)]\n",
    "    y_pred = modelo(x_test[indices])\n",
    "    tasaAcierto = sum([np.argmax(y_pred[i]) == np.argmax(y_test[indice]) for i, indice in enumerate(indices)])/numeroEjemplos\n",
    "    print(f'    La tasa de acierto es {tasaAcierto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e109cd60-a014-440e-b004-6c4dcb24b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    La tasa de acierto es 0.703\n"
     ]
    }
   ],
   "source": [
    "# descomentar para comprobar\n",
    "compruebaConTest(ResidualNetwork, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1cf62-a958-4e16-ab55-1ea5a41bb0fb",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 3$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, manteniendo fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se optimizado el entrenamiento, reduciendo al máximo el consumo de memoria.\n",
    " - (_-1 punto_) No se ha hecho uso de técnicas de DataAugmentation sobre las imágenes de entrenamiento.\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c0d445e-495c-49b0-bfbe-cc22b2c18dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primero que todo, tenemos que cargar los datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "885c0782-a973-4b6e-aef4-57e45f60218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_4/dense_2/kernel cannot be loaded\n",
      "sequential_4/dense_2/bias cannot be loaded\n",
      "sequential_4/dense_3/kernel cannot be loaded\n",
      "sequential_4/dense_3/bias cannot be loaded\n",
      ":) assinging conv2d_52/kernel to sequential_3/conv2d_44/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential_3/residual_block_18/batch_normalization_38/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential_3/residual_block_18/batch_normalization_38/beta\n",
      ":) assinging batch_normalization_38/moving_mean to sequential_3/residual_block_18/batch_normalization_38/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential_3/residual_block_18/batch_normalization_38/moving_variance\n",
      ":) assinging conv2d_54/kernel to sequential_3/residual_block_18/conv2d_45/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential_3/residual_block_18/batch_normalization_39/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential_3/residual_block_18/batch_normalization_39/beta\n",
      ":) assinging batch_normalization_39/moving_mean to sequential_3/residual_block_18/batch_normalization_39/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential_3/residual_block_18/batch_normalization_39/moving_variance\n",
      ":) assinging conv2d_55/kernel to sequential_3/residual_block_18/conv2d_46/kernel\n",
      ":) assinging conv2d_53/kernel to sequential_3/residual_block_18/conv2d_47/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential_3/residual_block_19/batch_normalization_40/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential_3/residual_block_19/batch_normalization_40/beta\n",
      ":) assinging batch_normalization_40/moving_mean to sequential_3/residual_block_19/batch_normalization_40/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential_3/residual_block_19/batch_normalization_40/moving_variance\n",
      ":) assinging conv2d_56/kernel to sequential_3/residual_block_19/conv2d_48/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential_3/residual_block_19/batch_normalization_41/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential_3/residual_block_19/batch_normalization_41/beta\n",
      ":) assinging batch_normalization_41/moving_mean to sequential_3/residual_block_19/batch_normalization_41/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential_3/residual_block_19/batch_normalization_41/moving_variance\n",
      ":) assinging conv2d_57/kernel to sequential_3/residual_block_19/conv2d_49/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential_3/residual_block_20/batch_normalization_42/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential_3/residual_block_20/batch_normalization_42/beta\n",
      ":) assinging batch_normalization_42/moving_mean to sequential_3/residual_block_20/batch_normalization_42/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential_3/residual_block_20/batch_normalization_42/moving_variance\n",
      ":) assinging conv2d_58/kernel to sequential_3/residual_block_20/conv2d_50/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential_3/residual_block_20/batch_normalization_43/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential_3/residual_block_20/batch_normalization_43/beta\n",
      ":) assinging batch_normalization_43/moving_mean to sequential_3/residual_block_20/batch_normalization_43/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential_3/residual_block_20/batch_normalization_43/moving_variance\n",
      ":) assinging conv2d_59/kernel to sequential_3/residual_block_20/conv2d_51/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential_3/residual_block_21/batch_normalization_44/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential_3/residual_block_21/batch_normalization_44/beta\n",
      ":) assinging batch_normalization_44/moving_mean to sequential_3/residual_block_21/batch_normalization_44/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential_3/residual_block_21/batch_normalization_44/moving_variance\n",
      ":) assinging conv2d_61/kernel to sequential_3/residual_block_21/conv2d_52/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential_3/residual_block_21/batch_normalization_45/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential_3/residual_block_21/batch_normalization_45/beta\n",
      ":) assinging batch_normalization_45/moving_mean to sequential_3/residual_block_21/batch_normalization_45/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential_3/residual_block_21/batch_normalization_45/moving_variance\n",
      ":) assinging conv2d_62/kernel to sequential_3/residual_block_21/conv2d_53/kernel\n",
      ":) assinging conv2d_60/kernel to sequential_3/residual_block_21/conv2d_54/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential_3/residual_block_22/batch_normalization_46/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential_3/residual_block_22/batch_normalization_46/beta\n",
      ":) assinging batch_normalization_46/moving_mean to sequential_3/residual_block_22/batch_normalization_46/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential_3/residual_block_22/batch_normalization_46/moving_variance\n",
      ":) assinging conv2d_63/kernel to sequential_3/residual_block_22/conv2d_55/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential_3/residual_block_22/batch_normalization_47/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential_3/residual_block_22/batch_normalization_47/beta\n",
      ":) assinging batch_normalization_47/moving_mean to sequential_3/residual_block_22/batch_normalization_47/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential_3/residual_block_22/batch_normalization_47/moving_variance\n",
      ":) assinging conv2d_64/kernel to sequential_3/residual_block_22/conv2d_56/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential_3/residual_block_23/batch_normalization_48/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential_3/residual_block_23/batch_normalization_48/beta\n",
      ":) assinging batch_normalization_48/moving_mean to sequential_3/residual_block_23/batch_normalization_48/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential_3/residual_block_23/batch_normalization_48/moving_variance\n",
      ":) assinging conv2d_65/kernel to sequential_3/residual_block_23/conv2d_57/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential_3/residual_block_23/batch_normalization_49/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential_3/residual_block_23/batch_normalization_49/beta\n",
      ":) assinging batch_normalization_49/moving_mean to sequential_3/residual_block_23/batch_normalization_49/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential_3/residual_block_23/batch_normalization_49/moving_variance\n",
      ":) assinging conv2d_66/kernel to sequential_3/residual_block_23/conv2d_58/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential_3/residual_block_24/batch_normalization_50/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential_3/residual_block_24/batch_normalization_50/beta\n",
      ":) assinging batch_normalization_50/moving_mean to sequential_3/residual_block_24/batch_normalization_50/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential_3/residual_block_24/batch_normalization_50/moving_variance\n",
      ":) assinging conv2d_68/kernel to sequential_3/residual_block_24/conv2d_59/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential_3/residual_block_24/batch_normalization_51/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential_3/residual_block_24/batch_normalization_51/beta\n",
      ":) assinging batch_normalization_51/moving_mean to sequential_3/residual_block_24/batch_normalization_51/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential_3/residual_block_24/batch_normalization_51/moving_variance\n",
      ":) assinging conv2d_69/kernel to sequential_3/residual_block_24/conv2d_60/kernel\n",
      ":) assinging conv2d_67/kernel to sequential_3/residual_block_24/conv2d_61/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential_3/residual_block_25/batch_normalization_52/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential_3/residual_block_25/batch_normalization_52/beta\n",
      ":) assinging batch_normalization_52/moving_mean to sequential_3/residual_block_25/batch_normalization_52/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential_3/residual_block_25/batch_normalization_52/moving_variance\n",
      ":) assinging conv2d_70/kernel to sequential_3/residual_block_25/conv2d_62/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential_3/residual_block_25/batch_normalization_53/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential_3/residual_block_25/batch_normalization_53/beta\n",
      ":) assinging batch_normalization_53/moving_mean to sequential_3/residual_block_25/batch_normalization_53/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential_3/residual_block_25/batch_normalization_53/moving_variance\n",
      ":) assinging conv2d_71/kernel to sequential_3/residual_block_25/conv2d_63/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential_3/residual_block_26/batch_normalization_54/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential_3/residual_block_26/batch_normalization_54/beta\n",
      ":) assinging batch_normalization_54/moving_mean to sequential_3/residual_block_26/batch_normalization_54/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential_3/residual_block_26/batch_normalization_54/moving_variance\n",
      ":) assinging conv2d_72/kernel to sequential_3/residual_block_26/conv2d_64/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential_3/residual_block_26/batch_normalization_55/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential_3/residual_block_26/batch_normalization_55/beta\n",
      ":) assinging batch_normalization_55/moving_mean to sequential_3/residual_block_26/batch_normalization_55/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential_3/residual_block_26/batch_normalization_55/moving_variance\n",
      ":) assinging conv2d_73/kernel to sequential_3/residual_block_26/conv2d_65/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential_3/batch_normalization_56/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential_3/batch_normalization_56/beta\n",
      ":) assinging batch_normalization_56/moving_mean to sequential_3/batch_normalization_56/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential_3/batch_normalization_56/moving_variance\n"
     ]
    }
   ],
   "source": [
    "# redefinimos el modelo\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.layers import RandomTranslation\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "from tensorflow.keras.layers import RandomContrast\n",
    "\n",
    "# extractor de caracteristicas\n",
    "\n",
    "aumentoDeDatos = Sequential(\n",
    "    [\n",
    "        RandomRotation(factor = .3), # rotaciones\n",
    "        RandomTranslation(height_factor = .2, width_factor = .2), # traslaciones\n",
    "        RandomFlip(), # giros\n",
    "        RandomContrast(factor = .2),  # contraste\n",
    "    ],\n",
    ")\n",
    "\n",
    "ResidualNetworkSinClasificador = Sequential([\n",
    "    #Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # lo unico que cambio es que la ultima capa, ahora no la tiene\n",
    "    # lo hacemos así porque de esta forma todo es no entrenable\n",
    "    \n",
    "])\n",
    "\n",
    "ResidualNetworkSinClasificador.trainable = False # que sea no entrenable\n",
    "\n",
    "Clasificador = Sequential( # definimos el clasificador\n",
    "    [\n",
    "        Dropout(rate = .05),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(10, activation = 'softmax') # Dense con salida el número de clases del dataset\n",
    "                                             # Softmax activation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# entrada\n",
    "input = Input(shape = (32,32,3))  # capa de input\n",
    "input = aumentoDeDatos(input) # le aplicamos el aumento de datos\n",
    "\n",
    "# salida\n",
    "output = ResidualNetworkSinClasificador(inputs = input)\n",
    "output = Clasificador(inputs = output)\n",
    "\n",
    "# modelo\n",
    "modeloEjercicio3 = Model(inputs = input, outputs = output, name = \"MobileNetV3Small\")\n",
    "load_weights(modeloEjercicio3, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df092082-9730-4bfc-8ff3-384b4bcbfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...No se ha entrenado la red sin hacer uso de la función fit, \n",
    "# entrenando el modelo con un bucle de entrenamiento desde cero...\n",
    "\n",
    "# definimos las funciones de entrenamiento\n",
    "# inspirándonos en el libro de François Chollet\n",
    "from math import ceil as techo\n",
    "from tensorflow import GradientTape \n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import reduce_mean\n",
    "from time import time as tiempo\n",
    "\n",
    "\n",
    "class GeneradorBatches:\n",
    "    \n",
    "    def __init__(self, imagenes, etiquetas, tamanoBatch = 128):\n",
    "        assert len(imagenes) == len(etiquetas)\n",
    "        self.indice = 0\n",
    "        self.imagenes = imagenes\n",
    "        self.etiquetas = etiquetas\n",
    "        self.tamanoBatch = tamanoBatch\n",
    "        self.numBatches = techo(len(imagenes)/tamanoBatch)\n",
    "\n",
    "    def siguiente(self):\n",
    "        imagenes = self.imagenes[self.indice : self.indice + self.tamanoBatch]\n",
    "        etiquetas = self.etiquetas[self.indice : self.indice + self.tamanoBatch]\n",
    "        self.indice += self.tamanoBatch\n",
    "        return imagenes, etiquetas\n",
    "\n",
    "def pasada(modelo, batchImagenes, batchEtiquetas, tasaAprendizaje):\n",
    "    \n",
    "    with GradientTape() as memoria:\n",
    "        predicciones = modelo(batchImagenes)\n",
    "        perdidasInstancia = losses.categorical_crossentropy(batchEtiquetas, predicciones)\n",
    "        perdidaMedia = reduce_mean(perdidasInstancia)\n",
    "    gradientes = memoria.gradient(perdidaMedia, modelo.trainable_weights)\n",
    "    optimizador = optimizers.Adam(learning_rate = tasaAprendizaje)\n",
    "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_weights))\n",
    "    \n",
    "    return perdidaMedia\n",
    "        \n",
    "def entrena(modelo, imagenes, etiquetas, epochs, tamanoBatch, tasaAprendizaje = 1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        antesEpoch = tiempo()\n",
    "        print(f'Epoch: {epoch}')\n",
    "        generadorBatches = GeneradorBatches(imagenes, etiquetas, tamanoBatch=tamanoBatch)\n",
    "        for batch in range(generadorBatches.numBatches):\n",
    "            antesBatch = tiempo()\n",
    "            batchImagenes, batchEtiquetas = generadorBatches.siguiente()\n",
    "            perdida = pasada(modelo, batchImagenes, batchEtiquetas, tasaAprendizaje)\n",
    "            \n",
    "            compruebaConTest(modelo, 512)\n",
    "            print(f'    Perdida en el batch {batch} = {perdida}')\n",
    "            \n",
    "            despuesBatch = tiempo()\n",
    "            print(f'    Batch #{batch}, tardo {despuesBatch - antesBatch} segundos\\n')\n",
    "        despuesEpoch = tiempo()\n",
    "        print(f'    Epoch #{epoch}, tardo {despuesEpoch - antesEpoch} segundos\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63907bae-e3cd-4f6d-8631-89169a6c1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    La tasa de acierto es 0.1640625\n",
      "    Perdida en el batch 0 = 2.3318581581115723\n",
      "    Batch #0, tardo 76.63116097450256 segundos\n",
      "    La tasa de acierto es 0.21875\n",
      "    Perdida en el batch 1 = 2.287306308746338\n",
      "    Batch #1, tardo 83.48607206344604 segundos\n",
      "    La tasa de acierto es 0.2109375\n",
      "    Perdida en el batch 2 = 2.234778881072998\n",
      "    Batch #2, tardo 96.55038714408875 segundos\n",
      "    La tasa de acierto es 0.21875\n",
      "    Perdida en el batch 3 = 2.1756327152252197\n",
      "    Batch #3, tardo 79.83383822441101 segundos\n",
      "    La tasa de acierto es 0.240234375\n",
      "    Perdida en el batch 4 = 2.156508445739746\n",
      "    Batch #4, tardo 86.56095099449158 segundos\n",
      "    La tasa de acierto es 0.287109375\n",
      "    Perdida en el batch 5 = 2.100123167037964\n",
      "    Batch #5, tardo 88.91169571876526 segundos\n",
      "    La tasa de acierto es 0.283203125\n",
      "    Perdida en el batch 6 = 2.0637598037719727\n",
      "    Batch #6, tardo 90.18079209327698 segundos\n",
      "    La tasa de acierto es 0.27734375\n",
      "    Perdida en el batch 7 = 2.0282912254333496\n",
      "    Batch #7, tardo 91.78363108634949 segundos\n",
      "    La tasa de acierto es 0.333984375\n",
      "    Perdida en el batch 8 = 1.9518204927444458\n",
      "    Batch #8, tardo 120.77968311309814 segundos\n",
      "    La tasa de acierto es 0.345703125\n",
      "    Perdida en el batch 9 = 1.941334843635559\n",
      "    Batch #9, tardo 74.7924497127533 segundos\n",
      "    La tasa de acierto es 0.357421875\n",
      "    Perdida en el batch 10 = 1.876949667930603\n",
      "    Batch #10, tardo 81.44734692573547 segundos\n",
      "    La tasa de acierto es 0.337890625\n",
      "    Perdida en el batch 11 = 1.836625337600708\n",
      "    Batch #11, tardo 90.25676584243774 segundos\n",
      "    La tasa de acierto es 0.37109375\n",
      "    Perdida en el batch 12 = 1.8049867153167725\n",
      "    Batch #12, tardo 78.62518572807312 segundos\n",
      "    La tasa de acierto es 0.42578125\n",
      "    Perdida en el batch 13 = 1.8007659912109375\n",
      "    Batch #13, tardo 90.91783094406128 segundos\n",
      "    La tasa de acierto es 0.439453125\n",
      "    Perdida en el batch 14 = 1.7503750324249268\n",
      "    Batch #14, tardo 79.9851062297821 segundos\n",
      "    La tasa de acierto es 0.42578125\n",
      "    Perdida en el batch 15 = 1.7165248394012451\n",
      "    Batch #15, tardo 76.82001304626465 segundos\n",
      "    La tasa de acierto es 0.396484375\n",
      "    Perdida en el batch 16 = 1.707519292831421\n",
      "    Batch #16, tardo 87.33012390136719 segundos\n",
      "    La tasa de acierto es 0.439453125\n",
      "    Perdida en el batch 17 = 1.645667314529419\n",
      "    Batch #17, tardo 78.82963991165161 segundos\n",
      "    La tasa de acierto es 0.400390625\n",
      "    Perdida en el batch 18 = 1.6221034526824951\n",
      "    Batch #18, tardo 86.4525420665741 segundos\n",
      "    La tasa de acierto es 0.46484375\n",
      "    Perdida en el batch 19 = 1.615661382675171\n",
      "    Batch #19, tardo 81.12693667411804 segundos\n"
     ]
    }
   ],
   "source": [
    "entrena(modeloEjercicio3, x_train, y_train, epochs=5, tamanoBatch=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603eadec-4b25-498c-9539-9439e05c0127",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 4$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, sin mantener fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se han mantenido congeladas las capas de BatchNormalization.\n",
    " - (_-1 punto_) No se ha hecho uso de un learning rate muy bajo, con el objetivo de no perder las capacidades de generalización de los pesos preentrenados\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce0a78-4afc-4ef8-a83d-4b16db7b5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descongelarModeloo(model):\n",
    "    # Descongelamos las capas, dejando BatchNormalization sin entrenar\n",
    "    for layer in modelo.layers:\n",
    "        if not isinstance(layer, BatchNormalization): # Comprueba que la capa no sea de tipo BatchNormalization\n",
    "            layer.trainable = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0e916-61cb-4e7a-a113-0bf3a2e69a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrena(modeloEjercicio3, x_train, y_train, epochs=5, tamanoBatch=2048,tasaAprendizaje=1e-5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca57ed-9991-4574-9515-55f55cdfed92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ent_AP",
   "language": "python",
   "name": "ent_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
