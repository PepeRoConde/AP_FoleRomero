{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b2a7e-7a28-4df6-989f-b2df7cb63fe6",
   "metadata": {},
   "source": [
    "# Redes de Neuronas con conexiones residuales y entrenados según _transfer learning_\n",
    "## Práctica 3\n",
    "\n",
    "#### Hugo Fole Avellás y José Romero Conde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a49d-451a-4f55-bbc7-4337319a9cf0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782e9-0a87-4281-96f2-d5157ae3acae",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 1$  \n",
    " ### (_2 puntos_) Define la capa ResidualBlock (ver Figura 2), usando como base la plantilla proporcionada en la Figura 3.\n",
    " - Ten en cuenta que el número de convoluciones depende de los valores de input_channels y out-put_channels.\n",
    " - Esta red no tiene capas de Pooling. La reducción del tamaño se realiza con el parámetro strides\n",
    "de las convoluciones, pero se modifica únicamente en 1 (o 2) de las convoluciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04184d0-0eaf-4b05-b7b8-705e21ca01bd",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4d6140-6e4b-4812-9c5b-6b69730b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, input_channels, output_channels, strides=(1, 1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.BN1 = layers.BatchNormalization()\n",
    "        self.Conv1 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = strides,\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "                                   \n",
    "        self.BN2 = layers.BatchNormalization()\n",
    "        self.Conv2 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = (1, 1),\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "        \n",
    "        if input_channels != output_channels:\n",
    "            self.salidaDistinta = True\n",
    "            self.ConvFuera = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (1, 1),\n",
    "                                   strides = strides,\n",
    "                                   use_bias=False)\n",
    "        else: self.salidaDistinta = False\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.BN1(x)\n",
    "        y = activations.silu(x)\n",
    "        x = self.Conv1(y)\n",
    "        x = self.BN2(x)\n",
    "        x = activations.silu(x)\n",
    "        x = self.Conv2(x)\n",
    "        if self.salidaDistinta:\n",
    "            y = self.ConvFuera(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138fe8-eb0f-4fbf-b55e-58246474cd34",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 2$  \n",
    "### (_2 puntos_) Define la red ResidualNetwork (ver Figura 1). Para comprobar su correcto funcionamiento haz lo siguiente:\n",
    " - Descarga los pesos del modelo preentrenado (los podrás encontrar en el canal de Teams de la asignatura).\n",
    " - Carga los pesos en tu modelo, haciendo uso de la función proporcionada en la Figura 4.\n",
    " - Comprueba que la precisión del modelo en CIFAR-100 es superior al 69 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c77615-3123-4342-af5c-75949f51bf14",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e2d100-07f8-4029-a634-0063bc4a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "ResidualNetwork = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(100,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22577e69-b36b-407c-ad07-fc4949ca0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_weights(model, weight_file):\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    all_vars = model.trainable_weights + model.non_trainable_weights\n",
    "    weight_list = [(x, weights[x]) for x in sorted(weights.keys())]\n",
    "    weights = {}\n",
    "    for i, var in enumerate(all_vars):\n",
    "        aux = var.path.split('/')[-2:]\n",
    "        classname = '_'.join(aux[0].split('_')[:-1])\n",
    "        name = aux[1]\n",
    "        assigned = False\n",
    "        for j, (key, value) in enumerate(weight_list):\n",
    "            if classname in key and name in key:\n",
    "                try:\n",
    "                    all_vars[i].assign(value)\n",
    "                    print(':) ',end='')\n",
    "                except:\n",
    "                    continue\n",
    "                print('assinging', key, 'to', var.path)\n",
    "                del weight_list[j]\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            #raise Exception(var.path + ' cannot be loaded')\n",
    "            print(var.path + ' cannot be loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84ade66-b928-42be-bd8a-469e7bda95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) assinging conv2d_52/kernel to sequential_1/conv2d_22/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential_1/residual_block_9/batch_normalization_19/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential_1/residual_block_9/batch_normalization_19/beta\n",
      ":) assinging conv2d_54/kernel to sequential_1/residual_block_9/conv2d_23/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential_1/residual_block_9/batch_normalization_20/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential_1/residual_block_9/batch_normalization_20/beta\n",
      ":) assinging conv2d_55/kernel to sequential_1/residual_block_9/conv2d_24/kernel\n",
      ":) assinging conv2d_53/kernel to sequential_1/residual_block_9/conv2d_25/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential_1/residual_block_10/batch_normalization_21/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential_1/residual_block_10/batch_normalization_21/beta\n",
      ":) assinging conv2d_56/kernel to sequential_1/residual_block_10/conv2d_26/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential_1/residual_block_10/batch_normalization_22/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential_1/residual_block_10/batch_normalization_22/beta\n",
      ":) assinging conv2d_57/kernel to sequential_1/residual_block_10/conv2d_27/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential_1/residual_block_11/batch_normalization_23/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential_1/residual_block_11/batch_normalization_23/beta\n",
      ":) assinging conv2d_58/kernel to sequential_1/residual_block_11/conv2d_28/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential_1/residual_block_11/batch_normalization_24/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential_1/residual_block_11/batch_normalization_24/beta\n",
      ":) assinging conv2d_59/kernel to sequential_1/residual_block_11/conv2d_29/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential_1/residual_block_12/batch_normalization_25/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential_1/residual_block_12/batch_normalization_25/beta\n",
      ":) assinging conv2d_61/kernel to sequential_1/residual_block_12/conv2d_30/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential_1/residual_block_12/batch_normalization_26/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential_1/residual_block_12/batch_normalization_26/beta\n",
      ":) assinging conv2d_62/kernel to sequential_1/residual_block_12/conv2d_31/kernel\n",
      ":) assinging conv2d_60/kernel to sequential_1/residual_block_12/conv2d_32/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential_1/residual_block_13/batch_normalization_27/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential_1/residual_block_13/batch_normalization_27/beta\n",
      ":) assinging conv2d_63/kernel to sequential_1/residual_block_13/conv2d_33/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential_1/residual_block_13/batch_normalization_28/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential_1/residual_block_13/batch_normalization_28/beta\n",
      ":) assinging conv2d_64/kernel to sequential_1/residual_block_13/conv2d_34/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential_1/residual_block_14/batch_normalization_29/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential_1/residual_block_14/batch_normalization_29/beta\n",
      ":) assinging conv2d_65/kernel to sequential_1/residual_block_14/conv2d_35/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential_1/residual_block_14/batch_normalization_30/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential_1/residual_block_14/batch_normalization_30/beta\n",
      ":) assinging conv2d_66/kernel to sequential_1/residual_block_14/conv2d_36/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential_1/residual_block_15/batch_normalization_31/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential_1/residual_block_15/batch_normalization_31/beta\n",
      ":) assinging conv2d_68/kernel to sequential_1/residual_block_15/conv2d_37/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential_1/residual_block_15/batch_normalization_32/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential_1/residual_block_15/batch_normalization_32/beta\n",
      ":) assinging conv2d_69/kernel to sequential_1/residual_block_15/conv2d_38/kernel\n",
      ":) assinging conv2d_67/kernel to sequential_1/residual_block_15/conv2d_39/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential_1/residual_block_16/batch_normalization_33/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential_1/residual_block_16/batch_normalization_33/beta\n",
      ":) assinging conv2d_70/kernel to sequential_1/residual_block_16/conv2d_40/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential_1/residual_block_16/batch_normalization_34/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential_1/residual_block_16/batch_normalization_34/beta\n",
      ":) assinging conv2d_71/kernel to sequential_1/residual_block_16/conv2d_41/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential_1/residual_block_17/batch_normalization_35/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential_1/residual_block_17/batch_normalization_35/beta\n",
      ":) assinging conv2d_72/kernel to sequential_1/residual_block_17/conv2d_42/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential_1/residual_block_17/batch_normalization_36/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential_1/residual_block_17/batch_normalization_36/beta\n",
      ":) assinging conv2d_73/kernel to sequential_1/residual_block_17/conv2d_43/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential_1/batch_normalization_37/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential_1/batch_normalization_37/beta\n",
      ":) assinging dense_2/kernel to sequential_1/dense_1/kernel\n",
      ":) assinging dense_2/bias to sequential_1/dense_1/bias\n",
      ":) assinging batch_normalization_38/moving_mean to sequential_1/residual_block_9/batch_normalization_19/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential_1/residual_block_9/batch_normalization_19/moving_variance\n",
      ":) assinging batch_normalization_39/moving_mean to sequential_1/residual_block_9/batch_normalization_20/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential_1/residual_block_9/batch_normalization_20/moving_variance\n",
      ":) assinging batch_normalization_40/moving_mean to sequential_1/residual_block_10/batch_normalization_21/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential_1/residual_block_10/batch_normalization_21/moving_variance\n",
      ":) assinging batch_normalization_41/moving_mean to sequential_1/residual_block_10/batch_normalization_22/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential_1/residual_block_10/batch_normalization_22/moving_variance\n",
      ":) assinging batch_normalization_42/moving_mean to sequential_1/residual_block_11/batch_normalization_23/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential_1/residual_block_11/batch_normalization_23/moving_variance\n",
      ":) assinging batch_normalization_43/moving_mean to sequential_1/residual_block_11/batch_normalization_24/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential_1/residual_block_11/batch_normalization_24/moving_variance\n",
      ":) assinging batch_normalization_44/moving_mean to sequential_1/residual_block_12/batch_normalization_25/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential_1/residual_block_12/batch_normalization_25/moving_variance\n",
      ":) assinging batch_normalization_45/moving_mean to sequential_1/residual_block_12/batch_normalization_26/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential_1/residual_block_12/batch_normalization_26/moving_variance\n",
      ":) assinging batch_normalization_46/moving_mean to sequential_1/residual_block_13/batch_normalization_27/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential_1/residual_block_13/batch_normalization_27/moving_variance\n",
      ":) assinging batch_normalization_47/moving_mean to sequential_1/residual_block_13/batch_normalization_28/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential_1/residual_block_13/batch_normalization_28/moving_variance\n",
      ":) assinging batch_normalization_48/moving_mean to sequential_1/residual_block_14/batch_normalization_29/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential_1/residual_block_14/batch_normalization_29/moving_variance\n",
      ":) assinging batch_normalization_49/moving_mean to sequential_1/residual_block_14/batch_normalization_30/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential_1/residual_block_14/batch_normalization_30/moving_variance\n",
      ":) assinging batch_normalization_50/moving_mean to sequential_1/residual_block_15/batch_normalization_31/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential_1/residual_block_15/batch_normalization_31/moving_variance\n",
      ":) assinging batch_normalization_51/moving_mean to sequential_1/residual_block_15/batch_normalization_32/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential_1/residual_block_15/batch_normalization_32/moving_variance\n",
      ":) assinging batch_normalization_52/moving_mean to sequential_1/residual_block_16/batch_normalization_33/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential_1/residual_block_16/batch_normalization_33/moving_variance\n",
      ":) assinging batch_normalization_53/moving_mean to sequential_1/residual_block_16/batch_normalization_34/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential_1/residual_block_16/batch_normalization_34/moving_variance\n",
      ":) assinging batch_normalization_54/moving_mean to sequential_1/residual_block_17/batch_normalization_35/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential_1/residual_block_17/batch_normalization_35/moving_variance\n",
      ":) assinging batch_normalization_55/moving_mean to sequential_1/residual_block_17/batch_normalization_36/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential_1/residual_block_17/batch_normalization_36/moving_variance\n",
      ":) assinging batch_normalization_56/moving_mean to sequential_1/batch_normalization_37/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential_1/batch_normalization_37/moving_variance\n"
     ]
    }
   ],
   "source": [
    "load_weights(ResidualNetwork, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c97ae3-b928-4cd7-b7b8-e014b768726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## carga y procesado de datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ec35da2-f31a-4a58-aaec-01eaadeee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobación de que la carga de pesos fue correcta\n",
    "from random import randint\n",
    "\n",
    "def compruebaConTest(modelo, numeroEjemplos):\n",
    "    indices = [randint(0,x_test.shape[0]-1) for _ in range(numeroEjemplos)]\n",
    "    y_pred = modelo(x_test[indices])\n",
    "    tasaAcierto = sum([np.argmax(y_pred[i]) == np.argmax(y_test[indice]) for i, indice in enumerate(indices)])/numeroEjemplos\n",
    "    print(f'    La tasa de acierto es {tasaAcierto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e109cd60-a014-440e-b004-6c4dcb24b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    La tasa de acierto es 0.703\n"
     ]
    }
   ],
   "source": [
    "# descomentar para comprobar\n",
    "compruebaConTest(ResidualNetwork, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1cf62-a958-4e16-ab55-1ea5a41bb0fb",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 3$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, manteniendo fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se optimizado el entrenamiento, reduciendo al máximo el consumo de memoria.\n",
    " - (_-1 punto_) No se ha hecho uso de técnicas de DataAugmentation sobre las imágenes de entrenamiento.\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c0d445e-495c-49b0-bfbe-cc22b2c18dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primero que todo, tenemos que cargar los datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "885c0782-a973-4b6e-aef4-57e45f60218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_4/dense_2/kernel cannot be loaded\n",
      "sequential_4/dense_2/bias cannot be loaded\n",
      "sequential_4/dense_3/kernel cannot be loaded\n",
      "sequential_4/dense_3/bias cannot be loaded\n",
      ":) assinging conv2d_52/kernel to sequential_3/conv2d_44/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential_3/residual_block_18/batch_normalization_38/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential_3/residual_block_18/batch_normalization_38/beta\n",
      ":) assinging batch_normalization_38/moving_mean to sequential_3/residual_block_18/batch_normalization_38/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential_3/residual_block_18/batch_normalization_38/moving_variance\n",
      ":) assinging conv2d_54/kernel to sequential_3/residual_block_18/conv2d_45/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential_3/residual_block_18/batch_normalization_39/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential_3/residual_block_18/batch_normalization_39/beta\n",
      ":) assinging batch_normalization_39/moving_mean to sequential_3/residual_block_18/batch_normalization_39/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential_3/residual_block_18/batch_normalization_39/moving_variance\n",
      ":) assinging conv2d_55/kernel to sequential_3/residual_block_18/conv2d_46/kernel\n",
      ":) assinging conv2d_53/kernel to sequential_3/residual_block_18/conv2d_47/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential_3/residual_block_19/batch_normalization_40/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential_3/residual_block_19/batch_normalization_40/beta\n",
      ":) assinging batch_normalization_40/moving_mean to sequential_3/residual_block_19/batch_normalization_40/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential_3/residual_block_19/batch_normalization_40/moving_variance\n",
      ":) assinging conv2d_56/kernel to sequential_3/residual_block_19/conv2d_48/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential_3/residual_block_19/batch_normalization_41/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential_3/residual_block_19/batch_normalization_41/beta\n",
      ":) assinging batch_normalization_41/moving_mean to sequential_3/residual_block_19/batch_normalization_41/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential_3/residual_block_19/batch_normalization_41/moving_variance\n",
      ":) assinging conv2d_57/kernel to sequential_3/residual_block_19/conv2d_49/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential_3/residual_block_20/batch_normalization_42/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential_3/residual_block_20/batch_normalization_42/beta\n",
      ":) assinging batch_normalization_42/moving_mean to sequential_3/residual_block_20/batch_normalization_42/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential_3/residual_block_20/batch_normalization_42/moving_variance\n",
      ":) assinging conv2d_58/kernel to sequential_3/residual_block_20/conv2d_50/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential_3/residual_block_20/batch_normalization_43/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential_3/residual_block_20/batch_normalization_43/beta\n",
      ":) assinging batch_normalization_43/moving_mean to sequential_3/residual_block_20/batch_normalization_43/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential_3/residual_block_20/batch_normalization_43/moving_variance\n",
      ":) assinging conv2d_59/kernel to sequential_3/residual_block_20/conv2d_51/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential_3/residual_block_21/batch_normalization_44/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential_3/residual_block_21/batch_normalization_44/beta\n",
      ":) assinging batch_normalization_44/moving_mean to sequential_3/residual_block_21/batch_normalization_44/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential_3/residual_block_21/batch_normalization_44/moving_variance\n",
      ":) assinging conv2d_61/kernel to sequential_3/residual_block_21/conv2d_52/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential_3/residual_block_21/batch_normalization_45/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential_3/residual_block_21/batch_normalization_45/beta\n",
      ":) assinging batch_normalization_45/moving_mean to sequential_3/residual_block_21/batch_normalization_45/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential_3/residual_block_21/batch_normalization_45/moving_variance\n",
      ":) assinging conv2d_62/kernel to sequential_3/residual_block_21/conv2d_53/kernel\n",
      ":) assinging conv2d_60/kernel to sequential_3/residual_block_21/conv2d_54/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential_3/residual_block_22/batch_normalization_46/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential_3/residual_block_22/batch_normalization_46/beta\n",
      ":) assinging batch_normalization_46/moving_mean to sequential_3/residual_block_22/batch_normalization_46/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential_3/residual_block_22/batch_normalization_46/moving_variance\n",
      ":) assinging conv2d_63/kernel to sequential_3/residual_block_22/conv2d_55/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential_3/residual_block_22/batch_normalization_47/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential_3/residual_block_22/batch_normalization_47/beta\n",
      ":) assinging batch_normalization_47/moving_mean to sequential_3/residual_block_22/batch_normalization_47/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential_3/residual_block_22/batch_normalization_47/moving_variance\n",
      ":) assinging conv2d_64/kernel to sequential_3/residual_block_22/conv2d_56/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential_3/residual_block_23/batch_normalization_48/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential_3/residual_block_23/batch_normalization_48/beta\n",
      ":) assinging batch_normalization_48/moving_mean to sequential_3/residual_block_23/batch_normalization_48/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential_3/residual_block_23/batch_normalization_48/moving_variance\n",
      ":) assinging conv2d_65/kernel to sequential_3/residual_block_23/conv2d_57/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential_3/residual_block_23/batch_normalization_49/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential_3/residual_block_23/batch_normalization_49/beta\n",
      ":) assinging batch_normalization_49/moving_mean to sequential_3/residual_block_23/batch_normalization_49/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential_3/residual_block_23/batch_normalization_49/moving_variance\n",
      ":) assinging conv2d_66/kernel to sequential_3/residual_block_23/conv2d_58/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential_3/residual_block_24/batch_normalization_50/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential_3/residual_block_24/batch_normalization_50/beta\n",
      ":) assinging batch_normalization_50/moving_mean to sequential_3/residual_block_24/batch_normalization_50/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential_3/residual_block_24/batch_normalization_50/moving_variance\n",
      ":) assinging conv2d_68/kernel to sequential_3/residual_block_24/conv2d_59/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential_3/residual_block_24/batch_normalization_51/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential_3/residual_block_24/batch_normalization_51/beta\n",
      ":) assinging batch_normalization_51/moving_mean to sequential_3/residual_block_24/batch_normalization_51/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential_3/residual_block_24/batch_normalization_51/moving_variance\n",
      ":) assinging conv2d_69/kernel to sequential_3/residual_block_24/conv2d_60/kernel\n",
      ":) assinging conv2d_67/kernel to sequential_3/residual_block_24/conv2d_61/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential_3/residual_block_25/batch_normalization_52/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential_3/residual_block_25/batch_normalization_52/beta\n",
      ":) assinging batch_normalization_52/moving_mean to sequential_3/residual_block_25/batch_normalization_52/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential_3/residual_block_25/batch_normalization_52/moving_variance\n",
      ":) assinging conv2d_70/kernel to sequential_3/residual_block_25/conv2d_62/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential_3/residual_block_25/batch_normalization_53/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential_3/residual_block_25/batch_normalization_53/beta\n",
      ":) assinging batch_normalization_53/moving_mean to sequential_3/residual_block_25/batch_normalization_53/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential_3/residual_block_25/batch_normalization_53/moving_variance\n",
      ":) assinging conv2d_71/kernel to sequential_3/residual_block_25/conv2d_63/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential_3/residual_block_26/batch_normalization_54/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential_3/residual_block_26/batch_normalization_54/beta\n",
      ":) assinging batch_normalization_54/moving_mean to sequential_3/residual_block_26/batch_normalization_54/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential_3/residual_block_26/batch_normalization_54/moving_variance\n",
      ":) assinging conv2d_72/kernel to sequential_3/residual_block_26/conv2d_64/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential_3/residual_block_26/batch_normalization_55/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential_3/residual_block_26/batch_normalization_55/beta\n",
      ":) assinging batch_normalization_55/moving_mean to sequential_3/residual_block_26/batch_normalization_55/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential_3/residual_block_26/batch_normalization_55/moving_variance\n",
      ":) assinging conv2d_73/kernel to sequential_3/residual_block_26/conv2d_65/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential_3/batch_normalization_56/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential_3/batch_normalization_56/beta\n",
      ":) assinging batch_normalization_56/moving_mean to sequential_3/batch_normalization_56/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential_3/batch_normalization_56/moving_variance\n"
     ]
    }
   ],
   "source": [
    "# redefinimos el modelo\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.layers import RandomTranslation\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "from tensorflow.keras.layers import RandomContrast\n",
    "\n",
    "# extractor de caracteristicas\n",
    "\n",
    "aumentoDeDatos = Sequential(\n",
    "    [\n",
    "        RandomRotation(factor = .3), # rotaciones\n",
    "        RandomTranslation(height_factor = .2, width_factor = .2), # traslaciones\n",
    "        RandomFlip(), # giros\n",
    "        RandomContrast(factor = .2),  # contraste\n",
    "    ],\n",
    ")\n",
    "\n",
    "ResidualNetworkSinClasificador = Sequential([\n",
    "    #Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # lo unico que cambio es que la ultima capa, ahora no la tiene\n",
    "    # lo hacemos así porque de esta forma todo es no entrenable\n",
    "    \n",
    "])\n",
    "\n",
    "ResidualNetworkSinClasificador.trainable = False # que sea no entrenable\n",
    "\n",
    "Clasificador = Sequential( # definimos el clasificador\n",
    "    [\n",
    "        Dropout(rate = .05),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(10, activation = 'softmax') # Dense con salida el número de clases del dataset\n",
    "                                             # Softmax activation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# entrada\n",
    "input = Input(shape = (32,32,3))  # capa de input\n",
    "input = aumentoDeDatos(input) # le aplicamos el aumento de datos\n",
    "\n",
    "# salida\n",
    "output = ResidualNetworkSinClasificador(inputs = input)\n",
    "output = Clasificador(inputs = output)\n",
    "\n",
    "# modelo\n",
    "modeloEjercicio3 = Model(inputs = input, outputs = output, name = \"MobileNetV3Small\")\n",
    "load_weights(modeloEjercicio3, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df092082-9730-4bfc-8ff3-384b4bcbfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...No se ha entrenado la red sin hacer uso de la función fit, \n",
    "# entrenando el modelo con un bucle de entrenamiento desde cero...\n",
    "\n",
    "# definimos las funciones de entrenamiento\n",
    "# inspirándonos en el libro de François Chollet\n",
    "from math import ceil as techo\n",
    "from tensorflow import GradientTape \n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import reduce_mean\n",
    "from time import time as tiempo\n",
    "\n",
    "\n",
    "class GeneradorBatches:\n",
    "    \n",
    "    def __init__(self, imagenes, etiquetas, tamanoBatch = 128):\n",
    "        assert len(imagenes) == len(etiquetas)\n",
    "        self.indice = 0\n",
    "        self.imagenes = imagenes\n",
    "        self.etiquetas = etiquetas\n",
    "        self.tamanoBatch = tamanoBatch\n",
    "        self.numBatches = techo(len(imagenes)/tamanoBatch)\n",
    "\n",
    "    def siguiente(self):\n",
    "        imagenes = self.imagenes[self.indice : self.indice + self.tamanoBatch]\n",
    "        etiquetas = self.etiquetas[self.indice : self.indice + self.tamanoBatch]\n",
    "        self.indice += self.tamanoBatch\n",
    "        return imagenes, etiquetas\n",
    "\n",
    "def pasada(modelo, batchImagenes, batchEtiquetas, tasaAprendizaje):\n",
    "    \n",
    "    with GradientTape() as memoria:\n",
    "        predicciones = modelo(batchImagenes)\n",
    "        perdidasInstancia = losses.categorical_crossentropy(batchEtiquetas, predicciones)\n",
    "        perdidaMedia = reduce_mean(perdidasInstancia)\n",
    "    gradientes = memoria.gradient(perdidaMedia, modelo.trainable_weights)\n",
    "    optimizador = optimizers.Adam(learning_rate = tasaAprendizaje)\n",
    "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_weights))\n",
    "    \n",
    "    return perdidaMedia\n",
    "        \n",
    "def entrena(modelo, imagenes, etiquetas, epochs, tamanoBatch, tasaAprendizaje = 1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        antesEpoch = tiempo()\n",
    "        print(f'Epoch: {epoch}')\n",
    "        generadorBatches = GeneradorBatches(imagenes, etiquetas, tamanoBatch=tamanoBatch)\n",
    "        for batch in range(generadorBatches.numBatches):\n",
    "            antesBatch = tiempo()\n",
    "            batchImagenes, batchEtiquetas = generadorBatches.siguiente()\n",
    "            perdida = pasada(modelo, batchImagenes, batchEtiquetas, tasaAprendizaje)\n",
    "            \n",
    "            compruebaConTest(modelo, 512)\n",
    "            print(f'    Perdida en el batch {batch} = {perdida}')\n",
    "            \n",
    "            despuesBatch = tiempo()\n",
    "            print(f'    Batch #{batch}, tardo {despuesBatch - antesBatch} segundos\\n')\n",
    "        despuesEpoch = tiempo()\n",
    "        print(f'    Epoch #{epoch}, tardo {despuesEpoch - antesEpoch} segundos\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63907bae-e3cd-4f6d-8631-89169a6c1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    La tasa de acierto es 0.1640625\n",
      "    Perdida en el batch 0 = 2.3318581581115723\n",
      "    Batch #0, tardo 76.63116097450256 segundos\n",
      "    La tasa de acierto es 0.21875\n",
      "    Perdida en el batch 1 = 2.287306308746338\n",
      "    Batch #1, tardo 83.48607206344604 segundos\n",
      "    La tasa de acierto es 0.2109375\n",
      "    Perdida en el batch 2 = 2.234778881072998\n",
      "    Batch #2, tardo 96.55038714408875 segundos\n",
      "    La tasa de acierto es 0.21875\n",
      "    Perdida en el batch 3 = 2.1756327152252197\n",
      "    Batch #3, tardo 79.83383822441101 segundos\n",
      "    La tasa de acierto es 0.240234375\n",
      "    Perdida en el batch 4 = 2.156508445739746\n",
      "    Batch #4, tardo 86.56095099449158 segundos\n",
      "    La tasa de acierto es 0.287109375\n",
      "    Perdida en el batch 5 = 2.100123167037964\n",
      "    Batch #5, tardo 88.91169571876526 segundos\n",
      "    La tasa de acierto es 0.283203125\n",
      "    Perdida en el batch 6 = 2.0637598037719727\n",
      "    Batch #6, tardo 90.18079209327698 segundos\n",
      "    La tasa de acierto es 0.27734375\n",
      "    Perdida en el batch 7 = 2.0282912254333496\n",
      "    Batch #7, tardo 91.78363108634949 segundos\n",
      "    La tasa de acierto es 0.333984375\n",
      "    Perdida en el batch 8 = 1.9518204927444458\n",
      "    Batch #8, tardo 120.77968311309814 segundos\n",
      "    La tasa de acierto es 0.345703125\n",
      "    Perdida en el batch 9 = 1.941334843635559\n",
      "    Batch #9, tardo 74.7924497127533 segundos\n",
      "    La tasa de acierto es 0.357421875\n",
      "    Perdida en el batch 10 = 1.876949667930603\n",
      "    Batch #10, tardo 81.44734692573547 segundos\n",
      "    La tasa de acierto es 0.337890625\n",
      "    Perdida en el batch 11 = 1.836625337600708\n",
      "    Batch #11, tardo 90.25676584243774 segundos\n",
      "    La tasa de acierto es 0.37109375\n",
      "    Perdida en el batch 12 = 1.8049867153167725\n",
      "    Batch #12, tardo 78.62518572807312 segundos\n",
      "    La tasa de acierto es 0.42578125\n",
      "    Perdida en el batch 13 = 1.8007659912109375\n",
      "    Batch #13, tardo 90.91783094406128 segundos\n",
      "    La tasa de acierto es 0.439453125\n",
      "    Perdida en el batch 14 = 1.7503750324249268\n",
      "    Batch #14, tardo 79.9851062297821 segundos\n",
      "    La tasa de acierto es 0.42578125\n",
      "    Perdida en el batch 15 = 1.7165248394012451\n",
      "    Batch #15, tardo 76.82001304626465 segundos\n",
      "    La tasa de acierto es 0.396484375\n",
      "    Perdida en el batch 16 = 1.707519292831421\n",
      "    Batch #16, tardo 87.33012390136719 segundos\n",
      "    La tasa de acierto es 0.439453125\n",
      "    Perdida en el batch 17 = 1.645667314529419\n",
      "    Batch #17, tardo 78.82963991165161 segundos\n",
      "    La tasa de acierto es 0.400390625\n",
      "    Perdida en el batch 18 = 1.6221034526824951\n",
      "    Batch #18, tardo 86.4525420665741 segundos\n",
      "    La tasa de acierto es 0.46484375\n",
      "    Perdida en el batch 19 = 1.615661382675171\n",
      "    Batch #19, tardo 81.12693667411804 segundos\n",
      "    La tasa de acierto es 0.474609375\n",
      "    Perdida en el batch 20 = 1.5743060111999512\n",
      "    Batch #20, tardo 94.65327429771423 segundos\n",
      "    La tasa de acierto es 0.482421875\n",
      "    Perdida en el batch 21 = 1.5537185668945312\n",
      "    Batch #21, tardo 86.17853903770447 segundos\n",
      "    La tasa de acierto es 0.48828125\n",
      "    Perdida en el batch 22 = 1.4995286464691162\n",
      "    Batch #22, tardo 81.47494387626648 segundos\n",
      "    La tasa de acierto es 0.490234375\n",
      "    Perdida en el batch 23 = 1.4833686351776123\n",
      "    Batch #23, tardo 82.85427117347717 segundos\n",
      "    La tasa de acierto es 0.4765625\n",
      "    Perdida en el batch 24 = 1.4003503322601318\n",
      "    Batch #24, tardo 28.39305806159973 segundos\n",
      "    Epoch #0, tardo 2094.857006788254 segundos\n",
      "Epoch: 1\n",
      "    La tasa de acierto es 0.494140625\n",
      "    Perdida en el batch 0 = 1.4523483514785767\n",
      "    Batch #0, tardo 87.20070099830627 segundos\n",
      "    La tasa de acierto es 0.529296875\n",
      "    Perdida en el batch 1 = 1.422767996788025\n",
      "    Batch #1, tardo 108.70034217834473 segundos\n",
      "    La tasa de acierto es 0.552734375\n",
      "    Perdida en el batch 2 = 1.4050428867340088\n",
      "    Batch #2, tardo 101.41925811767578 segundos\n",
      "    La tasa de acierto es 0.51953125\n",
      "    Perdida en el batch 3 = 1.3760721683502197\n",
      "    Batch #3, tardo 82.71653985977173 segundos\n",
      "    La tasa de acierto es 0.50390625\n",
      "    Perdida en el batch 4 = 1.3794306516647339\n",
      "    Batch #4, tardo 93.51718616485596 segundos\n",
      "    La tasa de acierto es 0.546875\n",
      "    Perdida en el batch 5 = 1.358295202255249\n",
      "    Batch #5, tardo 89.98858785629272 segundos\n",
      "    La tasa de acierto es 0.52734375\n",
      "    Perdida en el batch 6 = 1.3365650177001953\n",
      "    Batch #6, tardo 77.44397807121277 segundos\n",
      "    La tasa de acierto es 0.529296875\n",
      "    Perdida en el batch 7 = 1.3180439472198486\n",
      "    Batch #7, tardo 81.8092520236969 segundos\n",
      "    La tasa de acierto es 0.548828125\n",
      "    Perdida en el batch 8 = 1.2613375186920166\n",
      "    Batch #8, tardo 90.5935308933258 segundos\n",
      "    La tasa de acierto es 0.5234375\n",
      "    Perdida en el batch 9 = 1.2716509103775024\n",
      "    Batch #9, tardo 84.2909517288208 segundos\n",
      "    La tasa de acierto es 0.580078125\n",
      "    Perdida en el batch 10 = 1.2189645767211914\n",
      "    Batch #10, tardo 84.23131799697876 segundos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/1244835481.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentrena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeloEjercicio3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/2379635793.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, imagenes, etiquetas, epochs, tamanoBatch)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgeneradorBatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneradorBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtamanoBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumBatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mantesBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiempo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiguiente\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mperdida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpasada\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcompruebaConTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'    Perdida en el batch {batch} = {perdida}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/2379635793.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, batchImagenes, batchEtiquetas)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchImagenes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mperdidasInstancia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicciones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mperdidaMedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidasInstancia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mgradientes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidaMedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0moptimizador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradientes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*result_grad_components)\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_grads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         raise ValueError(\"Must return gradient for each variable from \"\n\u001b[1;32m    581\u001b[0m                          \"@custom_gradient grad_fn.\")\n\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0minput_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m       \u001b[0mvariable_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[1;32m    586\u001b[0m         nest.flatten(input_grads))\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dy)\u001b[0m\n\u001b[1;32m    470\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0msigmoid_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m       activation_grad = (\n\u001b[0;32m--> 474\u001b[0;31m           sigmoid_features * (1.0 + (beta * features) *\n\u001b[0m\u001b[1;32m    475\u001b[0m                               (1.0 - sigmoid_features)))\n\u001b[1;32m    476\u001b[0m       beta_grad = math_ops.reduce_sum(\n\u001b[1;32m    477\u001b[0m           \u001b[0mdy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_features\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/framework/override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# and the tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mul_dispatch_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[1;32m   1708\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[1;32m   1709\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m    \u001b[0;34m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \"\"\"\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6825\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[1;32m   6826\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6828\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6829\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6830\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6831\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6832\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entrena(modeloEjercicio3, x_train, y_train, epochs=5, tamanoBatch=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603eadec-4b25-498c-9539-9439e05c0127",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 4$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, sin mantener fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se han mantenido congeladas las capas de BatchNormalization.\n",
    " - (_-1 punto_) No se ha hecho uso de un learning rate muy bajo, con el objetivo de no perder las capacidades de generalización de los pesos preentrenados\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ce0a78-4afc-4ef8-a83d-4b16db7b5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descongelarModelo(modelo):\n",
    "    # Descongelamos las capas, dejando BatchNormalization sin entrenar\n",
    "    for layer in modelo.layers:\n",
    "        if not isinstance(layer, layers.BatchNormalization): # Comprueba que la capa no sea de tipo BatchNormalization\n",
    "            layer.trainable = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6c0e916-61cb-4e7a-a113-0bf3a2e69a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloEjercicio4 = modeloEjercicio3\n",
    "descongelarModelo(modeloEjercicio3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeca57ed-9991-4574-9515-55f55cdfed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pepe/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel', 'gamma', 'beta', 'kernel'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    La tasa de acierto es 0.537109375\n",
      "    Perdida en el batch 0 = 1.2422759532928467\n",
      "    Batch #0, tardo 38.57644176483154 segundos\n",
      "\n",
      "    La tasa de acierto es 0.55859375\n",
      "    Perdida en el batch 1 = 1.2344155311584473\n",
      "    Batch #1, tardo 33.88915777206421 segundos\n",
      "\n",
      "    La tasa de acierto es 0.564453125\n",
      "    Perdida en el batch 2 = 1.207159399986267\n",
      "    Batch #2, tardo 33.25633668899536 segundos\n",
      "\n",
      "    La tasa de acierto es 0.568359375\n",
      "    Perdida en el batch 3 = 1.2389841079711914\n",
      "    Batch #3, tardo 33.62417221069336 segundos\n",
      "\n",
      "    La tasa de acierto es 0.556640625\n",
      "    Perdida en el batch 4 = 1.2474092245101929\n",
      "    Batch #4, tardo 35.84189796447754 segundos\n",
      "\n",
      "    La tasa de acierto es 0.580078125\n",
      "    Perdida en el batch 5 = 1.229467749595642\n",
      "    Batch #5, tardo 36.913686990737915 segundos\n",
      "\n",
      "    La tasa de acierto es 0.599609375\n",
      "    Perdida en el batch 6 = 1.211355447769165\n",
      "    Batch #6, tardo 34.43661713600159 segundos\n",
      "\n",
      "    La tasa de acierto es 0.537109375\n",
      "    Perdida en el batch 7 = 1.2469334602355957\n",
      "    Batch #7, tardo 34.69858527183533 segundos\n",
      "\n",
      "    La tasa de acierto es 0.546875\n",
      "    Perdida en el batch 8 = 1.211431860923767\n",
      "    Batch #8, tardo 34.538413763046265 segundos\n",
      "\n",
      "    La tasa de acierto es 0.54296875\n",
      "    Perdida en el batch 9 = 1.277543306350708\n",
      "    Batch #9, tardo 33.86368989944458 segundos\n",
      "\n",
      "    La tasa de acierto es 0.51953125\n",
      "    Perdida en el batch 10 = 1.2653007507324219\n",
      "    Batch #10, tardo 36.23852014541626 segundos\n",
      "\n",
      "    La tasa de acierto es 0.595703125\n",
      "    Perdida en el batch 11 = 1.229053258895874\n",
      "    Batch #11, tardo 37.86200714111328 segundos\n",
      "\n",
      "    La tasa de acierto es 0.56640625\n",
      "    Perdida en el batch 12 = 1.1993807554244995\n",
      "    Batch #12, tardo 40.0566349029541 segundos\n",
      "\n",
      "    La tasa de acierto es 0.5625\n",
      "    Perdida en el batch 13 = 1.2843846082687378\n",
      "    Batch #13, tardo 36.46413588523865 segundos\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/422870593.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentrena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeloEjercicio3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtasaAprendizaje\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/3468269823.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, imagenes, etiquetas, epochs, tamanoBatch, tasaAprendizaje)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgeneradorBatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneradorBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtamanoBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumBatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mantesBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiempo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiguiente\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mperdida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpasada\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasaAprendizaje\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcompruebaConTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'    Perdida en el batch {batch} = {perdida}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_93423/3468269823.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, batchImagenes, batchEtiquetas, tasaAprendizaje)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchImagenes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mperdidasInstancia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicciones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mperdidaMedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidasInstancia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mgradientes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidaMedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0moptimizador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasaAprendizaje\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradientes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    580\u001b[0m   \u001b[0;31m# to use the nn_ops functions, we would have to convert `padding` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0;31m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m   \u001b[0;31m# in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m   return [\n\u001b[0;32m--> 584\u001b[0;31m       gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[1;32m    585\u001b[0m           \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1863\u001b[0m         data_format, \"dilations\", dilations)\n\u001b[1;32m   1864\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m       return conv2d_backprop_input_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entrena(modeloEjercicio3, x_train, y_train, epochs=5, tamanoBatch=1024,tasaAprendizaje=1e-5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a861e6-dd13-4216-9102-a572072b3854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ent_AP",
   "language": "python",
   "name": "ent_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
