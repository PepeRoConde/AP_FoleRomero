{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b2a7e-7a28-4df6-989f-b2df7cb63fe6",
   "metadata": {},
   "source": [
    "# Redes de Neuronas con conexiones residuales y entrenados según _transfer learning_\n",
    "## Práctica 3\n",
    "\n",
    "#### Hugo Fole Avellás y José Romero Conde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5164fd-0230-48eb-b9f1-5a6b8c279ac3",
   "metadata": {},
   "source": [
    "NOTA:\n",
    "\n",
    "La práctica tiene que por un lado programarse y por otro ejecutarse. Ocurrieron complicaciones que imposibiitaron la ejecución en múltipes ocasiones pero creemos que el código es correcto y esta completo. No hacemos en el ejercicio 3 ahorro de memoria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a49d-451a-4f55-bbc7-4337319a9cf0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782e9-0a87-4281-96f2-d5157ae3acae",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 1$  \n",
    " ### (_2 puntos_) Define la capa ResidualBlock (ver Figura 2), usando como base la plantilla proporcionada en la Figura 3.\n",
    " - Ten en cuenta que el número de convoluciones depende de los valores de input_channels y out-put_channels.\n",
    " - Esta red no tiene capas de Pooling. La reducción del tamaño se realiza con el parámetro strides\n",
    "de las convoluciones, pero se modifica únicamente en 1 (o 2) de las convoluciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04184d0-0eaf-4b05-b7b8-705e21ca01bd",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4d6140-6e4b-4812-9c5b-6b69730b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, input_channels, output_channels, strides=(1, 1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.BN1 = layers.BatchNormalization()\n",
    "        self.Conv1 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = strides,\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "                                   \n",
    "        self.BN2 = layers.BatchNormalization()\n",
    "        self.Conv2 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = (1, 1),\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "        \n",
    "        if input_channels != output_channels:\n",
    "            self.salidaDistinta = True\n",
    "            self.ConvFuera = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (1, 1),\n",
    "                                   strides = strides,\n",
    "                                   use_bias=False)\n",
    "        else: self.salidaDistinta = False\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.BN1(x)\n",
    "        y = activations.silu(x)\n",
    "        x = self.Conv1(y)\n",
    "        x = self.BN2(x)\n",
    "        x = activations.silu(x)\n",
    "        x = self.Conv2(x)\n",
    "        if self.salidaDistinta:\n",
    "            y = self.ConvFuera(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138fe8-eb0f-4fbf-b55e-58246474cd34",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 2$  \n",
    "### (_2 puntos_) Define la red ResidualNetwork (ver Figura 1). Para comprobar su correcto funcionamiento haz lo siguiente:\n",
    " - Descarga los pesos del modelo preentrenado (los podrás encontrar en el canal de Teams de la asignatura).\n",
    " - Carga los pesos en tu modelo, haciendo uso de la función proporcionada en la Figura 4.\n",
    " - Comprueba que la precisión del modelo en CIFAR-100 es superior al 69 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c77615-3123-4342-af5c-75949f51bf14",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e2d100-07f8-4029-a634-0063bc4a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "ResidualNetwork = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(100,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22577e69-b36b-407c-ad07-fc4949ca0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_weights(model, weight_file):\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    all_vars = model.trainable_weights + model.non_trainable_weights\n",
    "    weight_list = [(x, weights[x]) for x in sorted(weights.keys())]\n",
    "    weights = {}\n",
    "    for i, var in enumerate(all_vars):\n",
    "        aux = var.path.split('/')[-2:]\n",
    "        classname = '_'.join(aux[0].split('_')[:-1])\n",
    "        name = aux[1]\n",
    "        assigned = False\n",
    "        for j, (key, value) in enumerate(weight_list):\n",
    "            if classname in key and name in key:\n",
    "                try:\n",
    "                    all_vars[i].assign(value)\n",
    "                    print(':) ',end='')\n",
    "                except:\n",
    "                    continue\n",
    "                print('assinging', key, 'to', var.path)\n",
    "                del weight_list[j]\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            #raise Exception(var.path + ' cannot be loaded')\n",
    "            print(var.path + ' cannot be loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84ade66-b928-42be-bd8a-469e7bda95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) assinging conv2d_52/kernel to sequential/conv2d/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential/residual_block/batch_normalization/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential/residual_block/batch_normalization/beta\n",
      ":) assinging conv2d_54/kernel to sequential/residual_block/conv2d_1/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential/residual_block/batch_normalization_1/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential/residual_block/batch_normalization_1/beta\n",
      ":) assinging conv2d_55/kernel to sequential/residual_block/conv2d_2/kernel\n",
      ":) assinging conv2d_53/kernel to sequential/residual_block/conv2d_3/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential/residual_block_1/batch_normalization_2/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential/residual_block_1/batch_normalization_2/beta\n",
      ":) assinging conv2d_56/kernel to sequential/residual_block_1/conv2d_4/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential/residual_block_1/batch_normalization_3/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential/residual_block_1/batch_normalization_3/beta\n",
      ":) assinging conv2d_57/kernel to sequential/residual_block_1/conv2d_5/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential/residual_block_2/batch_normalization_4/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential/residual_block_2/batch_normalization_4/beta\n",
      ":) assinging conv2d_58/kernel to sequential/residual_block_2/conv2d_6/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential/residual_block_2/batch_normalization_5/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential/residual_block_2/batch_normalization_5/beta\n",
      ":) assinging conv2d_59/kernel to sequential/residual_block_2/conv2d_7/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential/residual_block_3/batch_normalization_6/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential/residual_block_3/batch_normalization_6/beta\n",
      ":) assinging conv2d_61/kernel to sequential/residual_block_3/conv2d_8/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential/residual_block_3/batch_normalization_7/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential/residual_block_3/batch_normalization_7/beta\n",
      ":) assinging conv2d_62/kernel to sequential/residual_block_3/conv2d_9/kernel\n",
      ":) assinging conv2d_60/kernel to sequential/residual_block_3/conv2d_10/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential/residual_block_4/batch_normalization_8/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential/residual_block_4/batch_normalization_8/beta\n",
      ":) assinging conv2d_63/kernel to sequential/residual_block_4/conv2d_11/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential/residual_block_4/batch_normalization_9/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential/residual_block_4/batch_normalization_9/beta\n",
      ":) assinging conv2d_64/kernel to sequential/residual_block_4/conv2d_12/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential/residual_block_5/batch_normalization_10/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential/residual_block_5/batch_normalization_10/beta\n",
      ":) assinging conv2d_65/kernel to sequential/residual_block_5/conv2d_13/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential/residual_block_5/batch_normalization_11/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential/residual_block_5/batch_normalization_11/beta\n",
      ":) assinging conv2d_66/kernel to sequential/residual_block_5/conv2d_14/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential/residual_block_6/batch_normalization_12/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential/residual_block_6/batch_normalization_12/beta\n",
      ":) assinging conv2d_68/kernel to sequential/residual_block_6/conv2d_15/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential/residual_block_6/batch_normalization_13/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential/residual_block_6/batch_normalization_13/beta\n",
      ":) assinging conv2d_69/kernel to sequential/residual_block_6/conv2d_16/kernel\n",
      ":) assinging conv2d_67/kernel to sequential/residual_block_6/conv2d_17/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential/residual_block_7/batch_normalization_14/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential/residual_block_7/batch_normalization_14/beta\n",
      ":) assinging conv2d_70/kernel to sequential/residual_block_7/conv2d_18/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential/residual_block_7/batch_normalization_15/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential/residual_block_7/batch_normalization_15/beta\n",
      ":) assinging conv2d_71/kernel to sequential/residual_block_7/conv2d_19/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential/residual_block_8/batch_normalization_16/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential/residual_block_8/batch_normalization_16/beta\n",
      ":) assinging conv2d_72/kernel to sequential/residual_block_8/conv2d_20/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential/residual_block_8/batch_normalization_17/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential/residual_block_8/batch_normalization_17/beta\n",
      ":) assinging conv2d_73/kernel to sequential/residual_block_8/conv2d_21/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential/batch_normalization_18/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential/batch_normalization_18/beta\n",
      ":) assinging dense_2/kernel to sequential/dense/kernel\n",
      ":) assinging dense_2/bias to sequential/dense/bias\n",
      ":) assinging batch_normalization_38/moving_mean to sequential/residual_block/batch_normalization/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential/residual_block/batch_normalization/moving_variance\n",
      ":) assinging batch_normalization_39/moving_mean to sequential/residual_block/batch_normalization_1/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential/residual_block/batch_normalization_1/moving_variance\n",
      ":) assinging batch_normalization_40/moving_mean to sequential/residual_block_1/batch_normalization_2/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential/residual_block_1/batch_normalization_2/moving_variance\n",
      ":) assinging batch_normalization_41/moving_mean to sequential/residual_block_1/batch_normalization_3/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential/residual_block_1/batch_normalization_3/moving_variance\n",
      ":) assinging batch_normalization_42/moving_mean to sequential/residual_block_2/batch_normalization_4/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential/residual_block_2/batch_normalization_4/moving_variance\n",
      ":) assinging batch_normalization_43/moving_mean to sequential/residual_block_2/batch_normalization_5/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential/residual_block_2/batch_normalization_5/moving_variance\n",
      ":) assinging batch_normalization_44/moving_mean to sequential/residual_block_3/batch_normalization_6/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential/residual_block_3/batch_normalization_6/moving_variance\n",
      ":) assinging batch_normalization_45/moving_mean to sequential/residual_block_3/batch_normalization_7/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential/residual_block_3/batch_normalization_7/moving_variance\n",
      ":) assinging batch_normalization_46/moving_mean to sequential/residual_block_4/batch_normalization_8/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential/residual_block_4/batch_normalization_8/moving_variance\n",
      ":) assinging batch_normalization_47/moving_mean to sequential/residual_block_4/batch_normalization_9/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential/residual_block_4/batch_normalization_9/moving_variance\n",
      ":) assinging batch_normalization_48/moving_mean to sequential/residual_block_5/batch_normalization_10/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential/residual_block_5/batch_normalization_10/moving_variance\n",
      ":) assinging batch_normalization_49/moving_mean to sequential/residual_block_5/batch_normalization_11/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential/residual_block_5/batch_normalization_11/moving_variance\n",
      ":) assinging batch_normalization_50/moving_mean to sequential/residual_block_6/batch_normalization_12/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential/residual_block_6/batch_normalization_12/moving_variance\n",
      ":) assinging batch_normalization_51/moving_mean to sequential/residual_block_6/batch_normalization_13/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential/residual_block_6/batch_normalization_13/moving_variance\n",
      ":) assinging batch_normalization_52/moving_mean to sequential/residual_block_7/batch_normalization_14/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential/residual_block_7/batch_normalization_14/moving_variance\n",
      ":) assinging batch_normalization_53/moving_mean to sequential/residual_block_7/batch_normalization_15/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential/residual_block_7/batch_normalization_15/moving_variance\n",
      ":) assinging batch_normalization_54/moving_mean to sequential/residual_block_8/batch_normalization_16/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential/residual_block_8/batch_normalization_16/moving_variance\n",
      ":) assinging batch_normalization_55/moving_mean to sequential/residual_block_8/batch_normalization_17/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential/residual_block_8/batch_normalization_17/moving_variance\n",
      ":) assinging batch_normalization_56/moving_mean to sequential/batch_normalization_18/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential/batch_normalization_18/moving_variance\n"
     ]
    }
   ],
   "source": [
    "load_weights(ResidualNetwork, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c97ae3-b928-4cd7-b7b8-e014b768726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## carga y procesado de datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec35da2-f31a-4a58-aaec-01eaadeee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobación de que la carga de pesos fue correcta\n",
    "from random import randint\n",
    "\n",
    "def compruebaConTest(modelo, numeroEjemplos):\n",
    "    indices = [randint(0,x_test.shape[0]-1) for _ in range(numeroEjemplos)]\n",
    "    y_pred = modelo(x_test[indices])\n",
    "    tasaAcierto = sum([np.argmax(y_pred[i]) == np.argmax(y_test[indice]) for i, indice in enumerate(indices)])/numeroEjemplos\n",
    "    print(f'    La tasa de acierto es {tasaAcierto}')\n",
    "    return tasaAcierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e109cd60-a014-440e-b004-6c4dcb24b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    La tasa de acierto es 0.7095\n"
     ]
    }
   ],
   "source": [
    "# descomentar para comprobar\n",
    "compruebaConTest(ResidualNetwork, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1cf62-a958-4e16-ab55-1ea5a41bb0fb",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 3$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, manteniendo fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se optimizado el entrenamiento, reduciendo al máximo el consumo de memoria.\n",
    " - (_-1 punto_) No se ha hecho uso de técnicas de DataAugmentation sobre las imágenes de entrenamiento.\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0d445e-495c-49b0-bfbe-cc22b2c18dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primero que todo, tenemos que cargar los datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885c0782-a973-4b6e-aef4-57e45f60218e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_3/dense_1/kernel cannot be loaded\n",
      "sequential_3/dense_1/bias cannot be loaded\n",
      "sequential_3/dense_2/kernel cannot be loaded\n",
      "sequential_3/dense_2/bias cannot be loaded\n",
      ":) assinging conv2d_52/kernel to sequential_2/conv2d_22/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential_2/residual_block_9/batch_normalization_19/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential_2/residual_block_9/batch_normalization_19/beta\n",
      ":) assinging batch_normalization_38/moving_mean to sequential_2/residual_block_9/batch_normalization_19/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential_2/residual_block_9/batch_normalization_19/moving_variance\n",
      ":) assinging conv2d_54/kernel to sequential_2/residual_block_9/conv2d_23/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential_2/residual_block_9/batch_normalization_20/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential_2/residual_block_9/batch_normalization_20/beta\n",
      ":) assinging batch_normalization_39/moving_mean to sequential_2/residual_block_9/batch_normalization_20/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential_2/residual_block_9/batch_normalization_20/moving_variance\n",
      ":) assinging conv2d_55/kernel to sequential_2/residual_block_9/conv2d_24/kernel\n",
      ":) assinging conv2d_53/kernel to sequential_2/residual_block_9/conv2d_25/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential_2/residual_block_10/batch_normalization_21/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential_2/residual_block_10/batch_normalization_21/beta\n",
      ":) assinging batch_normalization_40/moving_mean to sequential_2/residual_block_10/batch_normalization_21/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential_2/residual_block_10/batch_normalization_21/moving_variance\n",
      ":) assinging conv2d_56/kernel to sequential_2/residual_block_10/conv2d_26/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential_2/residual_block_10/batch_normalization_22/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential_2/residual_block_10/batch_normalization_22/beta\n",
      ":) assinging batch_normalization_41/moving_mean to sequential_2/residual_block_10/batch_normalization_22/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential_2/residual_block_10/batch_normalization_22/moving_variance\n",
      ":) assinging conv2d_57/kernel to sequential_2/residual_block_10/conv2d_27/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential_2/residual_block_11/batch_normalization_23/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential_2/residual_block_11/batch_normalization_23/beta\n",
      ":) assinging batch_normalization_42/moving_mean to sequential_2/residual_block_11/batch_normalization_23/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential_2/residual_block_11/batch_normalization_23/moving_variance\n",
      ":) assinging conv2d_58/kernel to sequential_2/residual_block_11/conv2d_28/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential_2/residual_block_11/batch_normalization_24/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential_2/residual_block_11/batch_normalization_24/beta\n",
      ":) assinging batch_normalization_43/moving_mean to sequential_2/residual_block_11/batch_normalization_24/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential_2/residual_block_11/batch_normalization_24/moving_variance\n",
      ":) assinging conv2d_59/kernel to sequential_2/residual_block_11/conv2d_29/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential_2/residual_block_12/batch_normalization_25/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential_2/residual_block_12/batch_normalization_25/beta\n",
      ":) assinging batch_normalization_44/moving_mean to sequential_2/residual_block_12/batch_normalization_25/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential_2/residual_block_12/batch_normalization_25/moving_variance\n",
      ":) assinging conv2d_61/kernel to sequential_2/residual_block_12/conv2d_30/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential_2/residual_block_12/batch_normalization_26/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential_2/residual_block_12/batch_normalization_26/beta\n",
      ":) assinging batch_normalization_45/moving_mean to sequential_2/residual_block_12/batch_normalization_26/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential_2/residual_block_12/batch_normalization_26/moving_variance\n",
      ":) assinging conv2d_62/kernel to sequential_2/residual_block_12/conv2d_31/kernel\n",
      ":) assinging conv2d_60/kernel to sequential_2/residual_block_12/conv2d_32/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential_2/residual_block_13/batch_normalization_27/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential_2/residual_block_13/batch_normalization_27/beta\n",
      ":) assinging batch_normalization_46/moving_mean to sequential_2/residual_block_13/batch_normalization_27/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential_2/residual_block_13/batch_normalization_27/moving_variance\n",
      ":) assinging conv2d_63/kernel to sequential_2/residual_block_13/conv2d_33/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential_2/residual_block_13/batch_normalization_28/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential_2/residual_block_13/batch_normalization_28/beta\n",
      ":) assinging batch_normalization_47/moving_mean to sequential_2/residual_block_13/batch_normalization_28/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential_2/residual_block_13/batch_normalization_28/moving_variance\n",
      ":) assinging conv2d_64/kernel to sequential_2/residual_block_13/conv2d_34/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential_2/residual_block_14/batch_normalization_29/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential_2/residual_block_14/batch_normalization_29/beta\n",
      ":) assinging batch_normalization_48/moving_mean to sequential_2/residual_block_14/batch_normalization_29/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential_2/residual_block_14/batch_normalization_29/moving_variance\n",
      ":) assinging conv2d_65/kernel to sequential_2/residual_block_14/conv2d_35/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential_2/residual_block_14/batch_normalization_30/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential_2/residual_block_14/batch_normalization_30/beta\n",
      ":) assinging batch_normalization_49/moving_mean to sequential_2/residual_block_14/batch_normalization_30/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential_2/residual_block_14/batch_normalization_30/moving_variance\n",
      ":) assinging conv2d_66/kernel to sequential_2/residual_block_14/conv2d_36/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential_2/residual_block_15/batch_normalization_31/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential_2/residual_block_15/batch_normalization_31/beta\n",
      ":) assinging batch_normalization_50/moving_mean to sequential_2/residual_block_15/batch_normalization_31/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential_2/residual_block_15/batch_normalization_31/moving_variance\n",
      ":) assinging conv2d_68/kernel to sequential_2/residual_block_15/conv2d_37/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential_2/residual_block_15/batch_normalization_32/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential_2/residual_block_15/batch_normalization_32/beta\n",
      ":) assinging batch_normalization_51/moving_mean to sequential_2/residual_block_15/batch_normalization_32/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential_2/residual_block_15/batch_normalization_32/moving_variance\n",
      ":) assinging conv2d_69/kernel to sequential_2/residual_block_15/conv2d_38/kernel\n",
      ":) assinging conv2d_67/kernel to sequential_2/residual_block_15/conv2d_39/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential_2/residual_block_16/batch_normalization_33/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential_2/residual_block_16/batch_normalization_33/beta\n",
      ":) assinging batch_normalization_52/moving_mean to sequential_2/residual_block_16/batch_normalization_33/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential_2/residual_block_16/batch_normalization_33/moving_variance\n",
      ":) assinging conv2d_70/kernel to sequential_2/residual_block_16/conv2d_40/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential_2/residual_block_16/batch_normalization_34/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential_2/residual_block_16/batch_normalization_34/beta\n",
      ":) assinging batch_normalization_53/moving_mean to sequential_2/residual_block_16/batch_normalization_34/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential_2/residual_block_16/batch_normalization_34/moving_variance\n",
      ":) assinging conv2d_71/kernel to sequential_2/residual_block_16/conv2d_41/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential_2/residual_block_17/batch_normalization_35/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential_2/residual_block_17/batch_normalization_35/beta\n",
      ":) assinging batch_normalization_54/moving_mean to sequential_2/residual_block_17/batch_normalization_35/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential_2/residual_block_17/batch_normalization_35/moving_variance\n",
      ":) assinging conv2d_72/kernel to sequential_2/residual_block_17/conv2d_42/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential_2/residual_block_17/batch_normalization_36/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential_2/residual_block_17/batch_normalization_36/beta\n",
      ":) assinging batch_normalization_55/moving_mean to sequential_2/residual_block_17/batch_normalization_36/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential_2/residual_block_17/batch_normalization_36/moving_variance\n",
      ":) assinging conv2d_73/kernel to sequential_2/residual_block_17/conv2d_43/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential_2/batch_normalization_37/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential_2/batch_normalization_37/beta\n",
      ":) assinging batch_normalization_56/moving_mean to sequential_2/batch_normalization_37/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential_2/batch_normalization_37/moving_variance\n"
     ]
    }
   ],
   "source": [
    "# redefinimos el modelo\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.layers import RandomTranslation\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "from tensorflow.keras.layers import RandomContrast\n",
    "\n",
    "# extractor de caracteristicas\n",
    "\n",
    "aumentoDeDatos = Sequential(\n",
    "    [\n",
    "        RandomRotation(factor = .3), # rotaciones\n",
    "        RandomTranslation(height_factor = .2, width_factor = .2), # traslaciones\n",
    "        RandomFlip(), # giros\n",
    "        RandomContrast(factor = .2),  # contraste\n",
    "    ],\n",
    ")\n",
    "\n",
    "ResidualNetworkSinClasificador = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # lo unico que cambio es que la ultima capa, ahora no la tiene\n",
    "    # lo hacemos así porque de esta forma todo es no entrenable\n",
    "    \n",
    "])\n",
    "'''\n",
    "\n",
    "load_weights(ResidualNetworkSinClasificador, \"p2_model_weights.pkl\")\n",
    "x_train = aumentoDeDatos(x_train)\n",
    "print('datos aumentados!')\n",
    "#x_train = ResidualNetworkSinClasificador(x_train)\n",
    "#print('características extraidas!')\n",
    "\n",
    "'''\n",
    "ResidualNetworkSinClasificador.trainable = False # que sea no entrenable\n",
    "\n",
    "Clasificador = Sequential( # definimos el clasificador\n",
    "    [\n",
    "        Dropout(rate = .05),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(10, activation = 'softmax') # Dense con salida el número de clases del dataset\n",
    "                                             # Softmax activation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# entrada\n",
    "input = Input(shape = (32,32,3))  # capa de input\n",
    "input = aumentoDeDatos(input) # le aplicamos el aumento de datos\n",
    "\n",
    "# salida\n",
    "outputs = ResidualNetworkSinClasificador(inputs = input)\n",
    "outputs = Clasificador(inputs = outputs)\n",
    "\n",
    "# modelo\n",
    "modeloEjercicio3 = Model(inputs = input, outputs = outputs, name = \"ejercicio3\")\n",
    "load_weights(modeloEjercicio3, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33216d0f-9b89-4384-aeb5-e94927ddb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import expand_dims\n",
    "# pasamos de los pixeles de las imagenes a las características\n",
    "#x_train = [ResidualNetworkSinClasificador(expand_dims(x_train[i], 0)) for i in range(x_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a619ddf-dce7-4e51-bb08-7366c95d23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import convert_to_tensor\n",
    "# pasamos de una lista de tensores a un tensor\n",
    "#x_train = convert_to_tensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df092082-9730-4bfc-8ff3-384b4bcbfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...No se ha entrenado la red sin hacer uso de la función fit, \n",
    "# entrenando el modelo con un bucle de entrenamiento desde cero...\n",
    "\n",
    "# definimos las funciones de entrenamiento\n",
    "# inspirándonos en el libro de François Chollet\n",
    "from math import inf\n",
    "from math import ceil as techo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import GradientTape \n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import reduce_mean\n",
    "from time import time as tiempo\n",
    "\n",
    "######################################################\n",
    "class GeneradorBatches:\n",
    "\n",
    "    def __init__(self, imagenes, etiquetas, tamanoBatch = 128):\n",
    "        assert len(imagenes) == len(etiquetas)\n",
    "        self.indice = 0\n",
    "        self.imagenes = imagenes\n",
    "        self.etiquetas = etiquetas\n",
    "        self.tamanoBatch = tamanoBatch\n",
    "        self.numBatches = techo(len(imagenes)/tamanoBatch)\n",
    "\n",
    "    def siguiente(self):\n",
    "        imagenes = self.imagenes[self.indice : self.indice + self.tamanoBatch]\n",
    "        etiquetas = self.etiquetas[self.indice : self.indice + self.tamanoBatch]\n",
    "        self.indice += self.tamanoBatch\n",
    "        return imagenes, etiquetas\n",
    "\n",
    "######################################################\n",
    "def pasada(modelo, batchImagenes, batchEtiquetas, learning_rate=1e-3):\n",
    "    \n",
    "    with GradientTape() as memoria:\n",
    "        predicciones = modelo(batchImagenes)\n",
    "        perdidasInstancia = losses.categorical_crossentropy(batchEtiquetas, predicciones)\n",
    "        perdidaMedia = reduce_mean(perdidasInstancia)\n",
    "    gradientes = memoria.gradient(perdidaMedia, modelo.trainable_weights)\n",
    "    optimizador = optimizers.Adam(learning_rate = learning_rate)\n",
    "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_weights))\n",
    "    \n",
    "    return perdidaMedia\n",
    "\n",
    "######################################################\n",
    "def entrena(modelo, imagenes, etiquetas, epochs, tamanoBatch, paradaTemprana='perdida', paciencilla = 15, learning_rate=1e-3):\n",
    "    #diccionario para almacenar cada loss y acc de cada epoch\n",
    "    hist = {\n",
    "        \"perdida_epoch\" : [],\n",
    "        \"precision_epoch\" : [],\n",
    "        \"perdida_batch\" : [],\n",
    "        \"precision_batch\" : []\n",
    "    }\n",
    "    \n",
    "    stop = 0\n",
    "    mejor_precision = 0.0\n",
    "    mejor_loss = inf\n",
    "    \n",
    "    for epoch in range(epochs):   \n",
    "        \n",
    "        if stop > paciencilla :\n",
    "            return hist\n",
    "        \n",
    "        antesEpoch = tiempo()\n",
    "        print(f'Epoch: {epoch}')\n",
    "        generadorBatches = GeneradorBatches(imagenes, etiquetas, tamanoBatch=tamanoBatch)\n",
    "        \n",
    "        for batch in range(generadorBatches.numBatches):\n",
    "            \n",
    "            antesBatch = tiempo()\n",
    "            batchImagenes, batchEtiquetas = generadorBatches.siguiente()\n",
    "            \n",
    "            perdida = pasada(modelo, batchImagenes, batchEtiquetas, learning_rate=learning_rate)\n",
    "            precision = compruebaConTest(modelo, 512)\n",
    "            \n",
    "            hist[\"perdida_batch\" ].append(perdida.numpy())\n",
    "            hist[\"precision_batch\" ].append(precision)\n",
    "            print(f'    Perdida en el batch {batch} = {perdida}')\n",
    "            \n",
    "            despuesBatch = tiempo()\n",
    "            print(f'    Batch #{batch}/{generadorBatches.numBatches}, tardo {despuesBatch - antesBatch} segundos\\n')\n",
    "            \n",
    "        hist[\"perdida_epoch\"].append(hist[\"perdida_batch\"][-1])\n",
    "        hist[\"precision_epoch\"].append(hist[\"precision_batch\"][-1])\n",
    "        \n",
    "        if paradaTemprana == \"precision\":\n",
    "            if mejor_precision < hist[\"precision_epoch\"][-1] : \n",
    "                mejor_precision = hist[\"precision_epoch\"][-1]\n",
    "                stop = 0\n",
    "            else: stop += 1 \n",
    "        elif paradaTemprana == \"perdida\":\n",
    "            if mejor_perdida > hist[\"perdida_epoch\"][-1] : \n",
    "                mejor_precision = hist[\"perdida_epoch\"][-1]\n",
    "                stop = 0\n",
    "            else: stop += 1\n",
    "        despuesEpoch = tiempo()\n",
    "        print(f'    Epoch #{epoch}, tardo {despuesEpoch - antesEpoch} segundos\\n')\n",
    "    return hist \n",
    "\n",
    "######################################################\n",
    "def graficar(hist, batches =  True):\n",
    "    if batches == True:\n",
    "        fig, axes = plt.subplots(4,1, figsize = (20,20))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2,1, figsize = (20,20))\n",
    "        \n",
    "    epochs = len(hist[\"precision_epoch\"])\n",
    "    x = np.arange(1,epochs,1)\n",
    "    axes[0].plot(x,hist[\"precision_epoch\"])\n",
    "    axes[0].set_title(\"Precisión x epochs\")\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Precisión')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(x,hist[\"perdida_epoch\"])\n",
    "    axes[1].set_title(\"Perdida x epochs\")\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Perdida')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    if batches == True:\n",
    "        \n",
    "        batches = len(hist[\"perdida_batch\"])\n",
    "        x = np.arange(1,batches,1)\n",
    "        \n",
    "        axes[2].plot(x,hist[\"precision_batch\"])\n",
    "        axes[2].set_title(\"Precisión x batches\")\n",
    "        axes[2].set_xlabel('Batches')\n",
    "        axes[2].set_ylabel('Precisión')\n",
    "        axes[2].legend()\n",
    "        \n",
    "        axes[3].plot(x,hist[\"perdida_batch\"])\n",
    "        axes[3].set_title(\"Perdida x batches\")\n",
    "        axes[3].set_xlabel('Batches')\n",
    "        axes[3].set_ylabel('Perdida')\n",
    "        axes[3].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06268c7e-4df3-4359-aade-4fe09e9b9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clasificador = Sequential( # definimos el clasificador\n",
    "    [\n",
    "        Input(shape=(256,)),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(10, activation = 'softmax') # Dense con salida el número de clases del dataset\n",
    "                                             # Softmax activation\n",
    "    ]\n",
    ")\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385f628d-d83a-4552-8835-ecaeff81a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.experimental.numpy import experimental_enable_numpy_behavior\n",
    "\n",
    "experimental_enable_numpy_behavior()\n",
    "\n",
    "x_train = x_train.reshape(50000,256)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63907bae-e3cd-4f6d-8631-89169a6c1b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    La tasa de acierto es 0.154296875\n",
      "    Perdida en el batch 0 = 2.5921428203582764\n",
      "    Batch #0/25, tardo 88.16404414176941 segundos\n",
      "\n",
      "    La tasa de acierto es 0.158203125\n",
      "    Perdida en el batch 1 = 2.5395426750183105\n",
      "    Batch #1/25, tardo 78.6652319431305 segundos\n",
      "\n",
      "    La tasa de acierto es 0.162109375\n",
      "    Perdida en el batch 2 = 2.4555680751800537\n",
      "    Batch #2/25, tardo 77.27769613265991 segundos\n",
      "\n",
      "    La tasa de acierto es 0.15625\n",
      "    Perdida en el batch 3 = 2.41371750831604\n",
      "    Batch #3/25, tardo 79.25289487838745 segundos\n",
      "\n",
      "    La tasa de acierto es 0.197265625\n",
      "    Perdida en el batch 4 = 2.349827766418457\n",
      "    Batch #4/25, tardo 74.78931093215942 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = entrena(modeloEjercicio3, x_train, y_train, epochs=2, tamanoBatch=2048,paradaTemprana = \"precision\", paciencilla=2, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97448248-6961-41a6-8eae-21bb4a77200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x_train[i].shape for i in range(x_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9342a16-c31c-4482-b842-19b52574d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720ecb9-795b-46e8-be29-171f37e13c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar(hist=hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603eadec-4b25-498c-9539-9439e05c0127",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 4$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, sin mantener fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se han mantenido congeladas las capas de BatchNormalization.\n",
    " - (_-1 punto_) No se ha hecho uso de un learning rate muy bajo, con el objetivo de no perder las capacidades de generalización de los pesos preentrenados\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d35c84c-e5bd-42c3-8bef-06a6e8da903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinimos el modelo\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import RandomTranslation\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "from tensorflow.keras.layers import RandomContrast\n",
    "\n",
    "# extractor de caracteristicas\n",
    "\n",
    "aumentoDeDatos = Sequential(\n",
    "    [\n",
    "        RandomRotation(factor = .3), # rotaciones\n",
    "        RandomTranslation(height_factor = .2, width_factor = .2), # traslaciones\n",
    "        RandomFlip(), # giros\n",
    "        RandomContrast(factor = .2),  # contraste\n",
    "    ],\n",
    ")\n",
    "\n",
    "ResidualNetworkSinClasificador = Sequential([\n",
    "    #Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # lo unico que cambio es que la ultima capa, ahora no la tiene\n",
    "    # lo hacemos así porque de esta forma todo es no entrenable\n",
    "    \n",
    "])\n",
    "\n",
    "ResidualNetworkSinClasificador.trainable = False # que sea no entrenable\n",
    "\n",
    "Clasificador = Sequential( # definimos el clasificador\n",
    "    [\n",
    "        Dropout(rate = .05),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dropout(rate = .05),\n",
    "        Dense(10, activation = 'softmax') # Dense con salida el número de clases del dataset\n",
    "                                             # Softmax activation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# entrada\n",
    "input = Input(shape = (32,32,3))  # capa de input\n",
    "input = aumentoDeDatos(input) # le aplicamos el aumento de datos\n",
    "\n",
    "# salida\n",
    "outputs = ResidualNetworkSinClasificador(inputs = input)\n",
    "outputs = Clasificador(inputs = outputs)\n",
    "\n",
    "# modelo\n",
    "modeloEjercicio3 = Model(inputs = input, outputs = outputs, name = \"ejercicio3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3ce0a78-4afc-4ef8-a83d-4b16db7b5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descongelarModelo(modelo):\n",
    "    # Descongelamos las capas, dejando BatchNormalization sin entrenar\n",
    "    for layer in modelo.layers:\n",
    "        if not isinstance(layer, BatchNormalization): # Comprueba que la capa no sea de tipo BatchNormalization\n",
    "            layer.trainable = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0e916-61cb-4e7a-a113-0bf3a2e69a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "descongelarModelo(modeloEjercicio3)\n",
    "hist2 = entrena(modeloEjercicio3, x_train, y_train, epochs=5, tamanoBatch=128,paradaTemprana = \"precision\", paciencilla=1, learning_rate=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca57ed-9991-4574-9515-55f55cdfed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar(hist=hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee754694-f1c1-48d2-a640-407de1af8200",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ent_AP",
   "language": "python",
   "name": "ent_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
