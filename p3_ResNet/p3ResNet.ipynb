{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b2a7e-7a28-4df6-989f-b2df7cb63fe6",
   "metadata": {},
   "source": [
    "# Redes de Neuronas con conexiones residuales y entrenados según _transfer learning_\n",
    "## Práctica 3\n",
    "\n",
    "#### Hugo Fole Avellás y José Romero Conde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a49d-451a-4f55-bbc7-4337319a9cf0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782e9-0a87-4281-96f2-d5157ae3acae",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 1$  \n",
    " ### (_2 puntos_) Define la capa ResidualBlock (ver Figura 2), usando como base la plantilla proporcionada en la Figura 3.\n",
    " - Ten en cuenta que el número de convoluciones depende de los valores de input_channels y out-put_channels.\n",
    " - Esta red no tiene capas de Pooling. La reducción del tamaño se realiza con el parámetro strides\n",
    "de las convoluciones, pero se modifica únicamente en 1 (o 2) de las convoluciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04184d0-0eaf-4b05-b7b8-705e21ca01bd",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4d6140-6e4b-4812-9c5b-6b69730b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, input_channels, output_channels, strides=(1, 1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.BN1 = layers.BatchNormalization()\n",
    "        self.Conv1 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = strides,\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "                                   \n",
    "        self.BN2 = layers.BatchNormalization()\n",
    "        self.Conv2 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = (1, 1),\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "        \n",
    "        if input_channels != output_channels:\n",
    "            self.salidaDistinta = True\n",
    "            self.ConvFuera = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (1, 1),\n",
    "                                   strides = strides,\n",
    "                                   use_bias=False)\n",
    "        else: self.salidaDistinta = False\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.BN1(x)\n",
    "        y = activations.silu(x)\n",
    "        x = self.Conv1(y)\n",
    "        x = self.BN2(x)\n",
    "        x = activations.silu(x)\n",
    "        x = self.Conv2(x)\n",
    "        if self.salidaDistinta:\n",
    "            y = self.ConvFuera(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138fe8-eb0f-4fbf-b55e-58246474cd34",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 2$  \n",
    "### (_2 puntos_) Define la red ResidualNetwork (ver Figura 1). Para comprobar su correcto funcionamiento haz lo siguiente:\n",
    " - Descarga los pesos del modelo preentrenado (los podrás encontrar en el canal de Teams de la asignatura).\n",
    " - Carga los pesos en tu modelo, haciendo uso de la función proporcionada en la Figura 4.\n",
    " - Comprueba que la precisión del modelo en CIFAR-100 es superior al 69 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c77615-3123-4342-af5c-75949f51bf14",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e2d100-07f8-4029-a634-0063bc4a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "ResidualNetwork = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(100,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22577e69-b36b-407c-ad07-fc4949ca0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_weights(model, weight_file):\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    all_vars = model.trainable_weights + model.non_trainable_weights\n",
    "    weight_list = [(x, weights[x]) for x in sorted(weights.keys())]\n",
    "    weights = {}\n",
    "    for i, var in enumerate(all_vars):\n",
    "        aux = var.path.split('/')[-2:]\n",
    "        classname = '_'.join(aux[0].split('_')[:-1])\n",
    "        name = aux[1]\n",
    "        assigned = False\n",
    "        for j, (key, value) in enumerate(weight_list):\n",
    "            if classname in key and name in key:\n",
    "                try:\n",
    "                    all_vars[i].assign(value)\n",
    "                    print(':) ',end='')\n",
    "                except:\n",
    "                    continue\n",
    "                print('assinging', key, 'to', var.path)\n",
    "                del weight_list[j]\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            raise Exception(var.path + ' cannot be loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84ade66-b928-42be-bd8a-469e7bda95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) assinging conv2d_52/kernel to sequential/conv2d/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential/residual_block/batch_normalization/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential/residual_block/batch_normalization/beta\n",
      ":) assinging conv2d_54/kernel to sequential/residual_block/conv2d_1/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential/residual_block/batch_normalization_1/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential/residual_block/batch_normalization_1/beta\n",
      ":) assinging conv2d_55/kernel to sequential/residual_block/conv2d_2/kernel\n",
      ":) assinging conv2d_53/kernel to sequential/residual_block/conv2d_3/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential/residual_block_1/batch_normalization_2/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential/residual_block_1/batch_normalization_2/beta\n",
      ":) assinging conv2d_56/kernel to sequential/residual_block_1/conv2d_4/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential/residual_block_1/batch_normalization_3/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential/residual_block_1/batch_normalization_3/beta\n",
      ":) assinging conv2d_57/kernel to sequential/residual_block_1/conv2d_5/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential/residual_block_2/batch_normalization_4/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential/residual_block_2/batch_normalization_4/beta\n",
      ":) assinging conv2d_58/kernel to sequential/residual_block_2/conv2d_6/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential/residual_block_2/batch_normalization_5/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential/residual_block_2/batch_normalization_5/beta\n",
      ":) assinging conv2d_59/kernel to sequential/residual_block_2/conv2d_7/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential/residual_block_3/batch_normalization_6/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential/residual_block_3/batch_normalization_6/beta\n",
      ":) assinging conv2d_61/kernel to sequential/residual_block_3/conv2d_8/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential/residual_block_3/batch_normalization_7/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential/residual_block_3/batch_normalization_7/beta\n",
      ":) assinging conv2d_62/kernel to sequential/residual_block_3/conv2d_9/kernel\n",
      ":) assinging conv2d_60/kernel to sequential/residual_block_3/conv2d_10/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential/residual_block_4/batch_normalization_8/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential/residual_block_4/batch_normalization_8/beta\n",
      ":) assinging conv2d_63/kernel to sequential/residual_block_4/conv2d_11/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential/residual_block_4/batch_normalization_9/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential/residual_block_4/batch_normalization_9/beta\n",
      ":) assinging conv2d_64/kernel to sequential/residual_block_4/conv2d_12/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential/residual_block_5/batch_normalization_10/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential/residual_block_5/batch_normalization_10/beta\n",
      ":) assinging conv2d_65/kernel to sequential/residual_block_5/conv2d_13/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential/residual_block_5/batch_normalization_11/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential/residual_block_5/batch_normalization_11/beta\n",
      ":) assinging conv2d_66/kernel to sequential/residual_block_5/conv2d_14/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential/residual_block_6/batch_normalization_12/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential/residual_block_6/batch_normalization_12/beta\n",
      ":) assinging conv2d_68/kernel to sequential/residual_block_6/conv2d_15/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential/residual_block_6/batch_normalization_13/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential/residual_block_6/batch_normalization_13/beta\n",
      ":) assinging conv2d_69/kernel to sequential/residual_block_6/conv2d_16/kernel\n",
      ":) assinging conv2d_67/kernel to sequential/residual_block_6/conv2d_17/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential/residual_block_7/batch_normalization_14/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential/residual_block_7/batch_normalization_14/beta\n",
      ":) assinging conv2d_70/kernel to sequential/residual_block_7/conv2d_18/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential/residual_block_7/batch_normalization_15/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential/residual_block_7/batch_normalization_15/beta\n",
      ":) assinging conv2d_71/kernel to sequential/residual_block_7/conv2d_19/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential/residual_block_8/batch_normalization_16/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential/residual_block_8/batch_normalization_16/beta\n",
      ":) assinging conv2d_72/kernel to sequential/residual_block_8/conv2d_20/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential/residual_block_8/batch_normalization_17/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential/residual_block_8/batch_normalization_17/beta\n",
      ":) assinging conv2d_73/kernel to sequential/residual_block_8/conv2d_21/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential/batch_normalization_18/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential/batch_normalization_18/beta\n",
      ":) assinging dense_2/kernel to sequential/dense/kernel\n",
      ":) assinging dense_2/bias to sequential/dense/bias\n",
      ":) assinging batch_normalization_38/moving_mean to sequential/residual_block/batch_normalization/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential/residual_block/batch_normalization/moving_variance\n",
      ":) assinging batch_normalization_39/moving_mean to sequential/residual_block/batch_normalization_1/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential/residual_block/batch_normalization_1/moving_variance\n",
      ":) assinging batch_normalization_40/moving_mean to sequential/residual_block_1/batch_normalization_2/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential/residual_block_1/batch_normalization_2/moving_variance\n",
      ":) assinging batch_normalization_41/moving_mean to sequential/residual_block_1/batch_normalization_3/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential/residual_block_1/batch_normalization_3/moving_variance\n",
      ":) assinging batch_normalization_42/moving_mean to sequential/residual_block_2/batch_normalization_4/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential/residual_block_2/batch_normalization_4/moving_variance\n",
      ":) assinging batch_normalization_43/moving_mean to sequential/residual_block_2/batch_normalization_5/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential/residual_block_2/batch_normalization_5/moving_variance\n",
      ":) assinging batch_normalization_44/moving_mean to sequential/residual_block_3/batch_normalization_6/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential/residual_block_3/batch_normalization_6/moving_variance\n",
      ":) assinging batch_normalization_45/moving_mean to sequential/residual_block_3/batch_normalization_7/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential/residual_block_3/batch_normalization_7/moving_variance\n",
      ":) assinging batch_normalization_46/moving_mean to sequential/residual_block_4/batch_normalization_8/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential/residual_block_4/batch_normalization_8/moving_variance\n",
      ":) assinging batch_normalization_47/moving_mean to sequential/residual_block_4/batch_normalization_9/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential/residual_block_4/batch_normalization_9/moving_variance\n",
      ":) assinging batch_normalization_48/moving_mean to sequential/residual_block_5/batch_normalization_10/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential/residual_block_5/batch_normalization_10/moving_variance\n",
      ":) assinging batch_normalization_49/moving_mean to sequential/residual_block_5/batch_normalization_11/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential/residual_block_5/batch_normalization_11/moving_variance\n",
      ":) assinging batch_normalization_50/moving_mean to sequential/residual_block_6/batch_normalization_12/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential/residual_block_6/batch_normalization_12/moving_variance\n",
      ":) assinging batch_normalization_51/moving_mean to sequential/residual_block_6/batch_normalization_13/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential/residual_block_6/batch_normalization_13/moving_variance\n",
      ":) assinging batch_normalization_52/moving_mean to sequential/residual_block_7/batch_normalization_14/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential/residual_block_7/batch_normalization_14/moving_variance\n",
      ":) assinging batch_normalization_53/moving_mean to sequential/residual_block_7/batch_normalization_15/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential/residual_block_7/batch_normalization_15/moving_variance\n",
      ":) assinging batch_normalization_54/moving_mean to sequential/residual_block_8/batch_normalization_16/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential/residual_block_8/batch_normalization_16/moving_variance\n",
      ":) assinging batch_normalization_55/moving_mean to sequential/residual_block_8/batch_normalization_17/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential/residual_block_8/batch_normalization_17/moving_variance\n",
      ":) assinging batch_normalization_56/moving_mean to sequential/batch_normalization_18/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential/batch_normalization_18/moving_variance\n"
     ]
    }
   ],
   "source": [
    "load_weights(ResidualNetwork, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c97ae3-b928-4cd7-b7b8-e014b768726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## carga y procesado de datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a7c33-d383-44da-b425-c6ab5b27d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.losses import CategoricalCrossentropy as CCE\n",
    "\n",
    "ResidualNetwork.compile(optimizer='adam',\n",
    "                        loss=CCE(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "ResidualNetwork.fit(x_train, y_train, \n",
    "                    epochs=5, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    batch_size=8)\n",
    "\n",
    "ResidualNetwork.evaluate(x_test)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec35da2-f31a-4a58-aaec-01eaadeee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ResidualNetwork(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c87a80-d839-45e0-96a3-736e5d574402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de acierto es 0.7029\n"
     ]
    }
   ],
   "source": [
    "n = 10_000\n",
    "tasaAcierto = sum([np.argmax(y_pred[i]) == np.argmax(y_test[i]) for i in range(n)])/n\n",
    "print(f'La tasa de acierto es {tasaAcierto}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1cf62-a958-4e16-ab55-1ea5a41bb0fb",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 3$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, manteniendo fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se optimizado el entrenamiento, reduciendo al máximo el consumo de memoria.\n",
    " - (_-1 punto_) No se ha hecho uso de técnicas de DataAugmentation sobre las imágenes de entrenamiento.\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df092082-9730-4bfc-8ff3-384b4bcbfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos las funciones de entrenamiento\n",
    "# inspirándonos en el libro de François Chollet\n",
    "from math import ceil as techo\n",
    "from tensorflow import GradientTape \n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import reduce_mean\n",
    "\n",
    "\n",
    "class GeneradorBatches:\n",
    "    \n",
    "    def __init__(self, imagenes, etiquetas, tamanoBatch = 128):\n",
    "        assert len(imagenes) == len(etiquetas)\n",
    "        self.indice = 0\n",
    "        self.imagenes = imagenes\n",
    "        self.etiquetas = etiquetas\n",
    "        self.tamanoBatch = tamanoBatch\n",
    "        self.numBatches = techo(len(imagenes)/tamanoBatch)\n",
    "\n",
    "    def siguiente(self):\n",
    "        imagenes = self.imagenes[self.indice : self.indice + self.tamanoBatch]\n",
    "        etiquetas = self.etiquetas[self.indice : self.indice + self.tamanoBatch]\n",
    "        self.indice += self.tamanoBatch\n",
    "        return imagenes, etiquetas\n",
    "\n",
    "def pasada(modelo, batchImagenes, batchEtiquetas):\n",
    "    \n",
    "    with GradientTape() as memoria:\n",
    "        predicciones = modelo(batchImagenes)\n",
    "        perdidasInstancia = losses.categorical_crossentropy(batchEtiquetas, predicciones)\n",
    "        perdidaMedia = reduce_mean(perdidasInstancia)\n",
    "    gradientes = memoria.gradient(perdidaMedia, modelo.trainable_weights)\n",
    "    optimizador = optimizers.Adam(learning_rate = 1e-3)\n",
    "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_weights))\n",
    "    \n",
    "    return perdidaMedia\n",
    "        \n",
    "def entrena(modelo, imagenes, etiquetas, epochs, tamanoBatch):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        generadorBatches = GeneradorBatches(imagenes, etiquetas, tamanoBatch=tamanoBatch)\n",
    "        for batch in range(generadorBatches.numBatches):\n",
    "            batchImagenes, batchEtiquetas = generadorBatches.siguiente()\n",
    "            perdida = pasada(modelo, batchImagenes, batchEtiquetas)\n",
    "            if epoch % 50 == 0:\n",
    "                print(f'    Perdida en el batch {batch} = {perdida}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63907bae-e3cd-4f6d-8631-89169a6c1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Perdida en el batch 0 = 79.14396667480469\n",
      "Perdida en el batch 1 = 60.081077575683594\n",
      "Perdida en el batch 2 = 8.607728958129883\n",
      "Perdida en el batch 3 = 20.795623779296875\n",
      "Perdida en el batch 4 = 12.961736679077148\n",
      "Perdida en el batch 5 = 13.240974426269531\n",
      "Perdida en el batch 6 = 9.057653427124023\n",
      "Perdida en el batch 7 = 28.29267120361328\n",
      "Perdida en el batch 8 = 32.88695526123047\n",
      "Perdida en el batch 9 = 22.417036056518555\n",
      "Perdida en el batch 10 = 10.818222045898438\n",
      "Perdida en el batch 11 = 8.224367141723633\n",
      "Perdida en el batch 12 = 20.226247787475586\n",
      "Perdida en el batch 13 = 13.231548309326172\n",
      "Perdida en el batch 14 = 12.950318336486816\n",
      "Perdida en el batch 15 = 6.2509236335754395\n",
      "Perdida en el batch 16 = 24.742847442626953\n",
      "Perdida en el batch 17 = 13.854986190795898\n",
      "Perdida en el batch 18 = 15.990713119506836\n",
      "Perdida en el batch 19 = 14.429088592529297\n",
      "Perdida en el batch 20 = 8.560184478759766\n",
      "Perdida en el batch 21 = 12.740371704101562\n",
      "Perdida en el batch 22 = 7.658596992492676\n",
      "Perdida en el batch 23 = 8.871072769165039\n",
      "Perdida en el batch 24 = 12.427544593811035\n",
      "Perdida en el batch 25 = 6.841117858886719\n",
      "Perdida en el batch 26 = 24.378726959228516\n",
      "Perdida en el batch 27 = 9.864946365356445\n",
      "Perdida en el batch 28 = 66.86726379394531\n",
      "Perdida en el batch 29 = 28.760181427001953\n",
      "Perdida en el batch 30 = 223.94485473632812\n",
      "Perdida en el batch 31 = 19.447711944580078\n",
      "Perdida en el batch 32 = 6.46558952331543\n",
      "Perdida en el batch 33 = 69.17510223388672\n",
      "Perdida en el batch 34 = 28.948040008544922\n",
      "Perdida en el batch 35 = 7.6225996017456055\n",
      "Perdida en el batch 36 = 116.51750946044922\n",
      "Perdida en el batch 37 = 12.53374195098877\n",
      "Perdida en el batch 38 = 9.088977813720703\n",
      "Perdida en el batch 39 = 9.46281909942627\n",
      "Perdida en el batch 40 = 8.789556503295898\n",
      "Perdida en el batch 41 = 8.023000717163086\n",
      "Perdida en el batch 42 = 17.569618225097656\n",
      "Perdida en el batch 43 = 6.993462562561035\n",
      "Perdida en el batch 44 = 38.02812194824219\n",
      "Perdida en el batch 45 = 16.727760314941406\n",
      "Perdida en el batch 46 = 31.375268936157227\n",
      "Perdida en el batch 47 = 6.073097229003906\n",
      "Perdida en el batch 48 = 7.596396446228027\n",
      "Perdida en el batch 49 = 7.513863563537598\n",
      "Perdida en el batch 50 = 668.9619140625\n",
      "Perdida en el batch 51 = 17.2413387298584\n",
      "Perdida en el batch 52 = 7.138343334197998\n",
      "Perdida en el batch 53 = 22.847761154174805\n",
      "Perdida en el batch 54 = 461.83111572265625\n",
      "Perdida en el batch 55 = 8.352516174316406\n",
      "Perdida en el batch 56 = 19.07309341430664\n",
      "Perdida en el batch 57 = 15.104134559631348\n",
      "Perdida en el batch 58 = 11.63340950012207\n",
      "Perdida en el batch 59 = 114.50472259521484\n",
      "Perdida en el batch 60 = 21.192951202392578\n",
      "Perdida en el batch 61 = 6.716710090637207\n",
      "Perdida en el batch 62 = 17.863059997558594\n",
      "Perdida en el batch 63 = 14.804213523864746\n",
      "Perdida en el batch 64 = 7.8756232261657715\n",
      "Perdida en el batch 65 = 8.522510528564453\n",
      "Perdida en el batch 66 = 5.646242618560791\n",
      "Perdida en el batch 67 = 22.24652862548828\n",
      "Perdida en el batch 68 = 12.396191596984863\n",
      "Perdida en el batch 69 = 7.3469696044921875\n",
      "Perdida en el batch 70 = 8.049610137939453\n",
      "Perdida en el batch 71 = 17.620389938354492\n",
      "Perdida en el batch 72 = 5.496463775634766\n",
      "Perdida en el batch 73 = 6.140538215637207\n",
      "Perdida en el batch 74 = 5.766026020050049\n",
      "Perdida en el batch 75 = 5.571399211883545\n",
      "Perdida en el batch 76 = 5.780747413635254\n",
      "Perdida en el batch 77 = 7.100450038909912\n",
      "Perdida en el batch 78 = 15.058744430541992\n",
      "Perdida en el batch 79 = 5.599643230438232\n",
      "Perdida en el batch 80 = 5.140059947967529\n",
      "Perdida en el batch 81 = 11.071200370788574\n",
      "Perdida en el batch 82 = 7.611795902252197\n",
      "Perdida en el batch 83 = 5.77676248550415\n",
      "Perdida en el batch 84 = 6.509130477905273\n",
      "Perdida en el batch 85 = 14744.5546875\n",
      "Perdida en el batch 86 = 5.412104606628418\n",
      "Perdida en el batch 87 = 5.733720779418945\n",
      "Perdida en el batch 88 = 263.3272705078125\n",
      "Perdida en el batch 89 = 5.66719388961792\n",
      "Perdida en el batch 90 = 5.303603649139404\n",
      "Perdida en el batch 91 = 5.684286117553711\n",
      "Perdida en el batch 92 = 6.388517379760742\n",
      "Perdida en el batch 93 = 5.10645866394043\n",
      "Perdida en el batch 94 = 11.469703674316406\n",
      "Perdida en el batch 95 = 5.573613166809082\n",
      "Perdida en el batch 96 = 4.8881120681762695\n",
      "Perdida en el batch 97 = 10.598328590393066\n",
      "Perdida en el batch 98 = 5.228295803070068\n",
      "Perdida en el batch 99 = 6.203945159912109\n",
      "Perdida en el batch 100 = 5.7681884765625\n",
      "Perdida en el batch 101 = 5.030060768127441\n",
      "Perdida en el batch 102 = 5.660083770751953\n",
      "Perdida en el batch 103 = 5.114823341369629\n",
      "Perdida en el batch 104 = 9.087944030761719\n",
      "Perdida en el batch 105 = 4.8211541175842285\n",
      "Perdida en el batch 106 = 13.634593963623047\n",
      "Perdida en el batch 107 = 37.62892150878906\n",
      "Perdida en el batch 108 = 14.440906524658203\n",
      "Perdida en el batch 109 = 4.784207344055176\n",
      "Perdida en el batch 110 = 5.571974754333496\n",
      "Perdida en el batch 111 = 5.742570400238037\n",
      "Perdida en el batch 112 = 4.933054447174072\n",
      "Perdida en el batch 113 = 6.073925971984863\n",
      "Perdida en el batch 114 = 6.805071830749512\n",
      "Perdida en el batch 115 = 13.140352249145508\n",
      "Perdida en el batch 116 = 6.27452278137207\n",
      "Perdida en el batch 117 = 5.210141181945801\n",
      "Perdida en el batch 118 = 6.553239345550537\n",
      "Perdida en el batch 119 = 5.144046783447266\n",
      "Perdida en el batch 120 = 5.961066246032715\n",
      "Perdida en el batch 121 = 4.734082221984863\n",
      "Perdida en el batch 122 = 5.1572160720825195\n",
      "Perdida en el batch 123 = 5.2014923095703125\n",
      "Perdida en el batch 124 = 5.518294334411621\n",
      "Perdida en el batch 125 = 4.962733745574951\n"
     ]
    }
   ],
   "source": [
    "entrena(ResidualNetwork, x_train, y_train, epochs=5, tamanoBatch=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca57ed-9991-4574-9515-55f55cdfed92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ent_AP",
   "language": "python",
   "name": "ent_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
