{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b2a7e-7a28-4df6-989f-b2df7cb63fe6",
   "metadata": {},
   "source": [
    "# Redes de Neuronas con conexiones residuales y entrenados según _transfer learning_\n",
    "## Práctica 3\n",
    "\n",
    "#### Hugo Fole Avellás y José Romero Conde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a49d-451a-4f55-bbc7-4337319a9cf0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782e9-0a87-4281-96f2-d5157ae3acae",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 1$  \n",
    " ### (_2 puntos_) Define la capa ResidualBlock (ver Figura 2), usando como base la plantilla proporcionada en la Figura 3.\n",
    " - Ten en cuenta que el número de convoluciones depende de los valores de input_channels y out-put_channels.\n",
    " - Esta red no tiene capas de Pooling. La reducción del tamaño se realiza con el parámetro strides\n",
    "de las convoluciones, pero se modifica únicamente en 1 (o 2) de las convoluciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04184d0-0eaf-4b05-b7b8-705e21ca01bd",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4d6140-6e4b-4812-9c5b-6b69730b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, input_channels, output_channels, strides=(1, 1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.BN1 = layers.BatchNormalization()\n",
    "        self.Conv1 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = strides,\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "                                   \n",
    "        self.BN2 = layers.BatchNormalization()\n",
    "        self.Conv2 = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (3, 3),\n",
    "                                   strides = (1, 1),\n",
    "                                   padding=\"same\",\n",
    "                                   use_bias=False)\n",
    "        \n",
    "        if input_channels != output_channels:\n",
    "            self.salidaDistinta = True\n",
    "            self.ConvFuera = layers.Conv2D(filters = output_channels, \n",
    "                                   kernel_size = (1, 1),\n",
    "                                   strides = strides,\n",
    "                                   use_bias=False)\n",
    "        else: self.salidaDistinta = False\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.BN1(x)\n",
    "        y = activations.silu(x)\n",
    "        x = self.Conv1(y)\n",
    "        x = self.BN2(x)\n",
    "        x = activations.silu(x)\n",
    "        x = self.Conv2(x)\n",
    "        if self.salidaDistinta:\n",
    "            y = self.ConvFuera(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138fe8-eb0f-4fbf-b55e-58246474cd34",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 2$  \n",
    "### (_2 puntos_) Define la red ResidualNetwork (ver Figura 1). Para comprobar su correcto funcionamiento haz lo siguiente:\n",
    " - Descarga los pesos del modelo preentrenado (los podrás encontrar en el canal de Teams de la asignatura).\n",
    " - Carga los pesos en tu modelo, haciendo uso de la función proporcionada en la Figura 4.\n",
    " - Comprueba que la precisión del modelo en CIFAR-100 es superior al 69 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c77615-3123-4342-af5c-75949f51bf14",
   "metadata": {},
   "source": [
    "!['arquitectura'](ResidualNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e2d100-07f8-4029-a634-0063bc4a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "ResidualNetwork = Sequential([\n",
    "    Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding=\"same\", use_bias=False), \n",
    "    # la configuración de la capa convolucional es para asegurarse que no se reduce tamaño\n",
    "    ResidualBlock(16,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,64),\n",
    "    ResidualBlock(64,128,strides=(2,2)),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,128),\n",
    "    ResidualBlock(128,256,strides=(2,2)),\n",
    "    ResidualBlock(256,256),\n",
    "    ResidualBlock(256,256),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.silu),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(100,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22577e69-b36b-407c-ad07-fc4949ca0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_weights(model, weight_file):\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "\n",
    "    all_vars = model.trainable_weights + model.non_trainable_weights\n",
    "    weight_list = [(x, weights[x]) for x in sorted(weights.keys())]\n",
    "    weights = {}\n",
    "    for i, var in enumerate(all_vars):\n",
    "        aux = var.path.split('/')[-2:]\n",
    "        classname = '_'.join(aux[0].split('_')[:-1])\n",
    "        name = aux[1]\n",
    "        assigned = False\n",
    "        for j, (key, value) in enumerate(weight_list):\n",
    "            if classname in key and name in key:\n",
    "                try:\n",
    "                    all_vars[i].assign(value)\n",
    "                    print(':) ',end='')\n",
    "                except:\n",
    "                    continue\n",
    "                print('assinging', key, 'to', var.path)\n",
    "                del weight_list[j]\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            raise Exception(var.path + ' cannot be loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84ade66-b928-42be-bd8a-469e7bda95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) assinging conv2d_52/kernel to sequential/conv2d/kernel\n",
      ":) assinging batch_normalization_38/gamma to sequential/residual_block/batch_normalization/gamma\n",
      ":) assinging batch_normalization_38/beta to sequential/residual_block/batch_normalization/beta\n",
      ":) assinging conv2d_54/kernel to sequential/residual_block/conv2d_1/kernel\n",
      ":) assinging batch_normalization_39/gamma to sequential/residual_block/batch_normalization_1/gamma\n",
      ":) assinging batch_normalization_39/beta to sequential/residual_block/batch_normalization_1/beta\n",
      ":) assinging conv2d_55/kernel to sequential/residual_block/conv2d_2/kernel\n",
      ":) assinging conv2d_53/kernel to sequential/residual_block/conv2d_3/kernel\n",
      ":) assinging batch_normalization_40/gamma to sequential/residual_block_1/batch_normalization_2/gamma\n",
      ":) assinging batch_normalization_40/beta to sequential/residual_block_1/batch_normalization_2/beta\n",
      ":) assinging conv2d_56/kernel to sequential/residual_block_1/conv2d_4/kernel\n",
      ":) assinging batch_normalization_41/gamma to sequential/residual_block_1/batch_normalization_3/gamma\n",
      ":) assinging batch_normalization_41/beta to sequential/residual_block_1/batch_normalization_3/beta\n",
      ":) assinging conv2d_57/kernel to sequential/residual_block_1/conv2d_5/kernel\n",
      ":) assinging batch_normalization_42/gamma to sequential/residual_block_2/batch_normalization_4/gamma\n",
      ":) assinging batch_normalization_42/beta to sequential/residual_block_2/batch_normalization_4/beta\n",
      ":) assinging conv2d_58/kernel to sequential/residual_block_2/conv2d_6/kernel\n",
      ":) assinging batch_normalization_43/gamma to sequential/residual_block_2/batch_normalization_5/gamma\n",
      ":) assinging batch_normalization_43/beta to sequential/residual_block_2/batch_normalization_5/beta\n",
      ":) assinging conv2d_59/kernel to sequential/residual_block_2/conv2d_7/kernel\n",
      ":) assinging batch_normalization_44/gamma to sequential/residual_block_3/batch_normalization_6/gamma\n",
      ":) assinging batch_normalization_44/beta to sequential/residual_block_3/batch_normalization_6/beta\n",
      ":) assinging conv2d_61/kernel to sequential/residual_block_3/conv2d_8/kernel\n",
      ":) assinging batch_normalization_45/gamma to sequential/residual_block_3/batch_normalization_7/gamma\n",
      ":) assinging batch_normalization_45/beta to sequential/residual_block_3/batch_normalization_7/beta\n",
      ":) assinging conv2d_62/kernel to sequential/residual_block_3/conv2d_9/kernel\n",
      ":) assinging conv2d_60/kernel to sequential/residual_block_3/conv2d_10/kernel\n",
      ":) assinging batch_normalization_46/gamma to sequential/residual_block_4/batch_normalization_8/gamma\n",
      ":) assinging batch_normalization_46/beta to sequential/residual_block_4/batch_normalization_8/beta\n",
      ":) assinging conv2d_63/kernel to sequential/residual_block_4/conv2d_11/kernel\n",
      ":) assinging batch_normalization_47/gamma to sequential/residual_block_4/batch_normalization_9/gamma\n",
      ":) assinging batch_normalization_47/beta to sequential/residual_block_4/batch_normalization_9/beta\n",
      ":) assinging conv2d_64/kernel to sequential/residual_block_4/conv2d_12/kernel\n",
      ":) assinging batch_normalization_48/gamma to sequential/residual_block_5/batch_normalization_10/gamma\n",
      ":) assinging batch_normalization_48/beta to sequential/residual_block_5/batch_normalization_10/beta\n",
      ":) assinging conv2d_65/kernel to sequential/residual_block_5/conv2d_13/kernel\n",
      ":) assinging batch_normalization_49/gamma to sequential/residual_block_5/batch_normalization_11/gamma\n",
      ":) assinging batch_normalization_49/beta to sequential/residual_block_5/batch_normalization_11/beta\n",
      ":) assinging conv2d_66/kernel to sequential/residual_block_5/conv2d_14/kernel\n",
      ":) assinging batch_normalization_50/gamma to sequential/residual_block_6/batch_normalization_12/gamma\n",
      ":) assinging batch_normalization_50/beta to sequential/residual_block_6/batch_normalization_12/beta\n",
      ":) assinging conv2d_68/kernel to sequential/residual_block_6/conv2d_15/kernel\n",
      ":) assinging batch_normalization_51/gamma to sequential/residual_block_6/batch_normalization_13/gamma\n",
      ":) assinging batch_normalization_51/beta to sequential/residual_block_6/batch_normalization_13/beta\n",
      ":) assinging conv2d_69/kernel to sequential/residual_block_6/conv2d_16/kernel\n",
      ":) assinging conv2d_67/kernel to sequential/residual_block_6/conv2d_17/kernel\n",
      ":) assinging batch_normalization_52/gamma to sequential/residual_block_7/batch_normalization_14/gamma\n",
      ":) assinging batch_normalization_52/beta to sequential/residual_block_7/batch_normalization_14/beta\n",
      ":) assinging conv2d_70/kernel to sequential/residual_block_7/conv2d_18/kernel\n",
      ":) assinging batch_normalization_53/gamma to sequential/residual_block_7/batch_normalization_15/gamma\n",
      ":) assinging batch_normalization_53/beta to sequential/residual_block_7/batch_normalization_15/beta\n",
      ":) assinging conv2d_71/kernel to sequential/residual_block_7/conv2d_19/kernel\n",
      ":) assinging batch_normalization_54/gamma to sequential/residual_block_8/batch_normalization_16/gamma\n",
      ":) assinging batch_normalization_54/beta to sequential/residual_block_8/batch_normalization_16/beta\n",
      ":) assinging conv2d_72/kernel to sequential/residual_block_8/conv2d_20/kernel\n",
      ":) assinging batch_normalization_55/gamma to sequential/residual_block_8/batch_normalization_17/gamma\n",
      ":) assinging batch_normalization_55/beta to sequential/residual_block_8/batch_normalization_17/beta\n",
      ":) assinging conv2d_73/kernel to sequential/residual_block_8/conv2d_21/kernel\n",
      ":) assinging batch_normalization_56/gamma to sequential/batch_normalization_18/gamma\n",
      ":) assinging batch_normalization_56/beta to sequential/batch_normalization_18/beta\n",
      ":) assinging dense_2/kernel to sequential/dense/kernel\n",
      ":) assinging dense_2/bias to sequential/dense/bias\n",
      ":) assinging batch_normalization_38/moving_mean to sequential/residual_block/batch_normalization/moving_mean\n",
      ":) assinging batch_normalization_38/moving_variance to sequential/residual_block/batch_normalization/moving_variance\n",
      ":) assinging batch_normalization_39/moving_mean to sequential/residual_block/batch_normalization_1/moving_mean\n",
      ":) assinging batch_normalization_39/moving_variance to sequential/residual_block/batch_normalization_1/moving_variance\n",
      ":) assinging batch_normalization_40/moving_mean to sequential/residual_block_1/batch_normalization_2/moving_mean\n",
      ":) assinging batch_normalization_40/moving_variance to sequential/residual_block_1/batch_normalization_2/moving_variance\n",
      ":) assinging batch_normalization_41/moving_mean to sequential/residual_block_1/batch_normalization_3/moving_mean\n",
      ":) assinging batch_normalization_41/moving_variance to sequential/residual_block_1/batch_normalization_3/moving_variance\n",
      ":) assinging batch_normalization_42/moving_mean to sequential/residual_block_2/batch_normalization_4/moving_mean\n",
      ":) assinging batch_normalization_42/moving_variance to sequential/residual_block_2/batch_normalization_4/moving_variance\n",
      ":) assinging batch_normalization_43/moving_mean to sequential/residual_block_2/batch_normalization_5/moving_mean\n",
      ":) assinging batch_normalization_43/moving_variance to sequential/residual_block_2/batch_normalization_5/moving_variance\n",
      ":) assinging batch_normalization_44/moving_mean to sequential/residual_block_3/batch_normalization_6/moving_mean\n",
      ":) assinging batch_normalization_44/moving_variance to sequential/residual_block_3/batch_normalization_6/moving_variance\n",
      ":) assinging batch_normalization_45/moving_mean to sequential/residual_block_3/batch_normalization_7/moving_mean\n",
      ":) assinging batch_normalization_45/moving_variance to sequential/residual_block_3/batch_normalization_7/moving_variance\n",
      ":) assinging batch_normalization_46/moving_mean to sequential/residual_block_4/batch_normalization_8/moving_mean\n",
      ":) assinging batch_normalization_46/moving_variance to sequential/residual_block_4/batch_normalization_8/moving_variance\n",
      ":) assinging batch_normalization_47/moving_mean to sequential/residual_block_4/batch_normalization_9/moving_mean\n",
      ":) assinging batch_normalization_47/moving_variance to sequential/residual_block_4/batch_normalization_9/moving_variance\n",
      ":) assinging batch_normalization_48/moving_mean to sequential/residual_block_5/batch_normalization_10/moving_mean\n",
      ":) assinging batch_normalization_48/moving_variance to sequential/residual_block_5/batch_normalization_10/moving_variance\n",
      ":) assinging batch_normalization_49/moving_mean to sequential/residual_block_5/batch_normalization_11/moving_mean\n",
      ":) assinging batch_normalization_49/moving_variance to sequential/residual_block_5/batch_normalization_11/moving_variance\n",
      ":) assinging batch_normalization_50/moving_mean to sequential/residual_block_6/batch_normalization_12/moving_mean\n",
      ":) assinging batch_normalization_50/moving_variance to sequential/residual_block_6/batch_normalization_12/moving_variance\n",
      ":) assinging batch_normalization_51/moving_mean to sequential/residual_block_6/batch_normalization_13/moving_mean\n",
      ":) assinging batch_normalization_51/moving_variance to sequential/residual_block_6/batch_normalization_13/moving_variance\n",
      ":) assinging batch_normalization_52/moving_mean to sequential/residual_block_7/batch_normalization_14/moving_mean\n",
      ":) assinging batch_normalization_52/moving_variance to sequential/residual_block_7/batch_normalization_14/moving_variance\n",
      ":) assinging batch_normalization_53/moving_mean to sequential/residual_block_7/batch_normalization_15/moving_mean\n",
      ":) assinging batch_normalization_53/moving_variance to sequential/residual_block_7/batch_normalization_15/moving_variance\n",
      ":) assinging batch_normalization_54/moving_mean to sequential/residual_block_8/batch_normalization_16/moving_mean\n",
      ":) assinging batch_normalization_54/moving_variance to sequential/residual_block_8/batch_normalization_16/moving_variance\n",
      ":) assinging batch_normalization_55/moving_mean to sequential/residual_block_8/batch_normalization_17/moving_mean\n",
      ":) assinging batch_normalization_55/moving_variance to sequential/residual_block_8/batch_normalization_17/moving_variance\n",
      ":) assinging batch_normalization_56/moving_mean to sequential/batch_normalization_18/moving_mean\n",
      ":) assinging batch_normalization_56/moving_variance to sequential/batch_normalization_18/moving_variance\n"
     ]
    }
   ],
   "source": [
    "load_weights(ResidualNetwork, \"p2_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c97ae3-b928-4cd7-b7b8-e014b768726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -1.0, 1.0, -1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## carga y procesado de datos\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import numpy as np\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "x_train = (2*x_train.astype(float)-255)/(255)\n",
    "x_test = (2*x_test.astype(float)-255)/(255)\n",
    "\n",
    "# como la salida de la red son 100 nodos se sobreentiende \n",
    "# que tengo que one-hot-ear Y\n",
    "\n",
    "y_train = np.zeros(shape=(Y_train.shape[0],max(Y_train)[0]+1))\n",
    "y_train[np.arange(Y_train.size),Y_train.T] = 1\n",
    "\n",
    "y_test = np.zeros(shape=(Y_test.shape[0],max(Y_test)[0]+1))\n",
    "y_test[np.arange(Y_test.size),Y_test.T] = 1\n",
    "\n",
    "(np.max(x_train),np.min(x_train),np.max(x_test),np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a7c33-d383-44da-b425-c6ab5b27d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.losses import CategoricalCrossentropy as CCE\n",
    "\n",
    "ResidualNetwork.compile(optimizer='adam',\n",
    "                        loss=CCE(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "ResidualNetwork.fit(x_train, y_train, \n",
    "                    epochs=5, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    batch_size=8)\n",
    "\n",
    "ResidualNetwork.evaluate(x_test)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec35da2-f31a-4a58-aaec-01eaadeee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ResidualNetwork(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c87a80-d839-45e0-96a3-736e5d574402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de acierto es 0.7029\n"
     ]
    }
   ],
   "source": [
    "n = 10_000\n",
    "tasaAcierto = sum([np.argmax(y_pred[i]) == np.argmax(y_test[i]) for i in range(n)])/n\n",
    "print(f'La tasa de acierto es {tasaAcierto}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1cf62-a958-4e16-ab55-1ea5a41bb0fb",
   "metadata": {},
   "source": [
    " ### $\\huge\\text{Ejercicio } 3$ \n",
    "### (_4 puntos_) Entrena, mediante la técnica de fine-tuning, sobre el dataset CIFAR-10, manteniendo fijos los pesos de la red preentrenada proporcionada en el canal de Teams de la asignatura. Analiza el resultado en el conjunto de test. Se penalizarán los siguientes puntos:\n",
    " - (_-4 puntos_) No se ha entrenado la red correctamente, y no se proporciona el resultado obtenido en\n",
    "el conjunto de test.\n",
    " - (_-1 punto_) No se ha utilizado la red original completa, a excepción de la última capa.\n",
    " - (_-1 punto_) No se optimizado el entrenamiento, reduciendo al máximo el consumo de memoria.\n",
    " - (_-1 punto_) No se ha hecho uso de técnicas de DataAugmentation sobre las imágenes de entrenamiento.\n",
    " - (_-2 puntos_) No se ha entrenado la red sin hacer uso de la función fit, entrenando el modelo con un\n",
    "bucle de entrenamiento desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df092082-9730-4bfc-8ff3-384b4bcbfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos las funciones de entrenamiento\n",
    "# inspirándonos en el libro de François Chollet\n",
    "from math import ceil as techo\n",
    "from tensorflow import GradientTape \n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import reduce_mean\n",
    "\n",
    "\n",
    "class GeneradorBatches:\n",
    "    \n",
    "    def __init__(self, imagenes, etiquetas, tamanoBatch = 128):\n",
    "        assert len(imagenes) == len(etiquetas)\n",
    "        self.indice = 0\n",
    "        self.imagenes = imagenes\n",
    "        self.etiquetas = etiquetas\n",
    "        self.tamanoBatch = tamanoBatch\n",
    "        self.numBatches = techo(len(imagenes)/tamanoBatch)\n",
    "\n",
    "    def siguiente(self):\n",
    "        imagenes = self.imagenes[self.indice : self.indice + self.tamanoBatch]\n",
    "        etiquetas = self.etiquetas[self.indice : self.indice + self.tamanoBatch]\n",
    "        self.indice += self.tamanoBatch\n",
    "        return imagenes, etiquetas\n",
    "\n",
    "def pasada(modelo, batchImagenes, batchEtiquetas):\n",
    "    \n",
    "    with GradientTape() as memoria:\n",
    "        predicciones = modelo(batchImagenes)\n",
    "        perdidasInstancia = losses.categorical_crossentropy(batchEtiquetas, predicciones)\n",
    "        perdidaMedia = reduce_mean(perdidasInstancia)\n",
    "    gradientes = memoria.gradient(perdidaMedia, modelo.trainable_weights)\n",
    "    optimizador = optimizers.Adam(learning_rate = 1e-3)\n",
    "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_weights))\n",
    "    \n",
    "    return perdidaMedia\n",
    "        \n",
    "def entrena(modelo, imagenes, etiquetas, epochs, tamanoBatch):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        generadorBatches = GeneradorBatches(imagenes, etiquetas, tamanoBatch=tamanoBatch)\n",
    "        for batch in range(generadorBatches.numBatches):\n",
    "            batchImagenes, batchEtiquetas = generadorBatches.siguiente()\n",
    "            perdida = pasada(modelo, batchImagenes, batchEtiquetas)\n",
    "            if batch % 50 == 0:\n",
    "                n = 1000\n",
    "                predicciones = modelo(x_test[:n])\n",
    "                tasaAcierto = sum([np.argmax(predicciones[i]) == np.argmax(y_test[i]) for i in range(n)])/n\n",
    "                print(f'La tasa de acierto es {tasaAcierto}')\n",
    "        \n",
    "                print(f'    Perdida en el batch {batch} = {perdida}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63907bae-e3cd-4f6d-8631-89169a6c1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "La tasa de acierto es 0.031\n",
      "    Perdida en el batch 0 = 4.066461086273193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_55250/2304241136.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentrena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResidualNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_55250/3096668201.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, imagenes, etiquetas, epochs, tamanoBatch)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgeneradorBatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneradorBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanoBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtamanoBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumBatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneradorBatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiguiente\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mperdida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpasada\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchImagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v6/ybkmv61n1fgd69w84cn8vtqc0000gn/T/ipykernel_55250/3096668201.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modelo, batchImagenes, batchEtiquetas)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchImagenes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mperdidasInstancia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchEtiquetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicciones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mperdidaMedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidasInstancia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgradientes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdidaMedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradientes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*result_grad_components)\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_grads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         raise ValueError(\"Must return gradient for each variable from \"\n\u001b[1;32m    581\u001b[0m                          \"@custom_gradient grad_fn.\")\n\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0minput_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m       \u001b[0mvariable_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[1;32m    586\u001b[0m         nest.flatten(input_grads))\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dy)\u001b[0m\n\u001b[1;32m    473\u001b[0m       activation_grad = (\n\u001b[1;32m    474\u001b[0m           sigmoid_features * (1.0 + (beta * features) *\n\u001b[1;32m    475\u001b[0m                               (1.0 - sigmoid_features)))\n\u001b[1;32m    476\u001b[0m       beta_grad = math_ops.reduce_sum(\n\u001b[0;32m--> 477\u001b[0;31m           \u001b[0mdy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_features\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m           (1.0 - sigmoid_features))\n\u001b[1;32m    479\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivation_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversidadedaCoruña/carrera/3/1/ap/Ent_AP/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  12056\u001b[0m         _ctx, \"Square\", name, x)\n\u001b[1;32m  12057\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12058\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12059\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12060\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  12061\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12062\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12063\u001b[0m       _result = _dispatcher_for_square(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entrena(ResidualNetwork, x_train, y_train, epochs=5, tamanoBatch=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca57ed-9991-4574-9515-55f55cdfed92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ent_AP",
   "language": "python",
   "name": "ent_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
