


import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt





from keras.datasets import cifar10

(x_train, y_train) , (x_test, y_test) = cifar10.load_data()








y_train = keras.utils.to_categorical(y_train, num_classes = 10)
y_test = keras.utils.to_categorical(y_test, num_classes = 10)





x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train = x_train / 255.0
x_test = x_test / 255.0





def plot(train, validation, title):
    plt.clf()
    epochs = range(1, len(train) + 1)

    plt.plot(epochs, train, 'b-o', label='Training ' + title)
    plt.plot(epochs, validation, 'r--o', label='Validation '+ title)

    plt.title('Training and validation ' + title)
    plt.xlabel('Epochs')
    plt.ylabel(title)
    plt.legend()
    plt.show()







from keras import layers





inputs = keras.Input(shape=(32, 32, 3))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=4, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

#Compile
model.compile(optimizer="adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=50, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')














inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters=10, kernel_size=3,activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=50, kernel_size=3 ,padding = "same", activation="relu")(x)
x = layers.MaxPooling2D(pool_size=3,padding="same")(x)
x = layers.Conv2D(filters=100, kernel_size=4, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding ="same")(x)
x = layers.Conv2D(filters=300, kernel_size=2,padding="same", activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding ="same")(x)
x = layers.Conv2D(filters=600, kernel_size=3,padding="same", activation="relu")(x)

x = layers.Flatten()(x)


outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

#Compile
model.compile(optimizer="RMSProp",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1, epochs=20, batch_size=64)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")











inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu")(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = layers.BatchNormalization()(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same")(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Flatten()(x)

outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()
#Compile
model.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')








from keras import regularizers


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Flatten()(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()
#Compile
model.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        start_from_epoch=5)
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=50, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')











inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.05)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.05)(x)

x = layers.Flatten()(x)
x = layers.Dense(500,activation = "relu")(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_11 = keras.Model(inputs=inputs, outputs=outputs)

model_11.summary()
#Compile
model_11.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history_11 = model_11.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=128,callbacks = callbacks)


loss, acc = model_11.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")
plot(history_11.history['loss'], history_11.history['val_loss'], 'loss')
plot(history_11.history['categorical_accuracy'], history_11.history['val_categorical_accuracy'], 'categorical_accuracy')








inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Flatten()(x)
x = layers.Dense(512,activation = "relu")(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_12 = keras.Model(inputs=inputs, outputs=outputs)

model_12.summary()
#Compile
model_12.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",

        patience=5,
        start_from_epoch=5)
]
#Fitting
history_12 = model_12.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=256,callbacks = callbacks)


loss, acc = model_12.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")
plot(history_12.history['loss'], history_12.history['val_loss'], 'loss')
plot(history_12.history['categorical_accuracy'], history_12.history['val_categorical_accuracy'], 'categorical_accuracy')#





inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 64, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 64, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Conv2D(filters=128 , kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.15)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.15)(x)

x = x = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Flatten()(x)
x = layers.Dense(256,activation = "relu")(x)
x = layers.Dropout(0.15)(x)
x = layers.Dense(64,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_13 = keras.Model(inputs=inputs, outputs=outputs)

model_13.summary()
#Compile
model_13.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=10,
        start_from_epoch=10)
]
#Fitting
history_13 = model_13.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=1024,callbacks = callbacks)

loss, acc = model_13.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")
plot(history_13.history['loss'], history_13.history['val_loss'], 'loss')
plot(history_13.history['categorical_accuracy'], history_13.history['val_categorical_accuracy'], 'categorical_accuracy')#








data_augmentation = keras.Sequential(
[
    layers.RandomFlip('horizontal'),               # de que forma flipea
    layers.RandomRotation(0.1)                  # de que forma rota
])


inputs = keras.Input(shape=(32, 32, 3))

x2 = data_augmentation(inputs)
x = layers.Conv2D(filters = 64, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x2)
x = layers.Conv2D(filters = 64, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Conv2D(filters=128 , kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.15)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.15)(x)

x = x = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Flatten()(x)
x = layers.Dense(256,activation = "relu")(x)
x = layers.Dropout(0.15)(x)
x = layers.Dense(64,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model2 = keras.Model(inputs=inputs, outputs=outputs)


model2.summary()
#Compile
model2.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=15,
        start_from_epoch=5,
        restore_best_weights=True
        )
]
#Fitting
history2 = model2.fit(x_train, y_train,validation_split = 0.3 ,epochs=200, batch_size=1024,callbacks = callbacks)


















