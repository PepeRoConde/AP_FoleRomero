


import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt





from keras.datasets import cifar10

(x_train, y_train) , (x_test, y_test) = cifar10.load_data()








y_train = keras.utils.to_categorical(y_train, num_classes = 10)
y_test = keras.utils.to_categorical(y_test, num_classes = 10)





x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train = x_train / 255.0
x_test = x_test / 255.0





def plot(train, validation, title):
    plt.clf()
    epochs = range(1, len(train) + 1)
    
    plt.plot(epochs, train, 'b-o', label='Training ' + title)
    plt.plot(epochs, validation, 'r--o', label='Validation '+ title) 

    plt.title('Training and validation ' + title)
    plt.xlabel('Epochs')
    plt.ylabel(title)
    plt.legend()
    plt.show()
    






from keras import layers





inputs = keras.Input(shape=(32, 32, 3))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=4, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

#Compile
model.compile(optimizer="adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=50, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')


model.summary()










##Cambiar esto un poquillo / intentar otras distribuciones

inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters=10, kernel_size=5,activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=50, kernel_size=3 ,padding = "same", activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding="same")(x)
x = layers.Conv2D(filters=100, kernel_size=2,strides = (2,2), activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding ="same")(x)
x = layers.Conv2D(filters=300, kernel_size=2, activation="relu")(x)
x = layers.Flatten()(x)


outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

#Compile
model.compile(optimizer="adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Fitting
history = model.fit(x_train, y_train, epochs=20, batch_size=64)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters=10, kernel_size=3,activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=50, kernel_size=3 ,padding = "same", activation="relu")(x)
x = layers.MaxPooling2D(pool_size=3,padding="same")(x)
x = layers.Conv2D(filters=100, kernel_size=4, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding ="same")(x)
x = layers.Conv2D(filters=300, kernel_size=2,padding="same", activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2,padding ="same")(x)
x = layers.Conv2D(filters=600, kernel_size=3,padding="same", activation="relu")(x)

x = layers.Flatten()(x)


outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

#Compile
model.compile(optimizer="RMSProp",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1, epochs=20, batch_size=64)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")





inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu")(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = layers.BatchNormalization()(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same")(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Flatten()(x)

outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()
#Compile
model.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')





from tensorflow.keras import regularizers


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Flatten()(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()
#Compile
model.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)


loss, acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")


plot(history.history['loss'], history.history['val_loss'], 'loss')
plot(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy')





inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 128, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.3)(x)

x = layers.Conv2D(filters=256, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-2))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.3)(x)

x = x = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x)
x = x = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.3)(x)

x = layers.Flatten()(x)
x = layers.Dense(512,activation = "relu")(x)
x = layers.Dense(256,activation = "relu")(x)
x = layers.Dense(128,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()
#Compile
model.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        start_from_epoch=5)
]
#Fitting
history = model.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)











inputs2 = keras.Input(shape=(32, 32, 3))

x2 = layers.Conv2D(filters = 128, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(inputs2)
x2 = layers.MaxPooling2D(pool_size=2)(x2)
x2 = layers.Dropout(0.3)(x2)

x2 = layers.Conv2D(filters=256, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-2))(x2)
x2 = layers.MaxPooling2D(pool_size=2)(x2)
x2 = layers.Dropout(0.3)(x2)

x2 = x2 = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x2)
x2 = x2 = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x2)
x2 = x2 = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x2)
x2 = layers.MaxPooling2D(pool_size=2)(x2)
x2 = layers.Dropout(0.3)(x2)

x2 = layers.Flatten()(x2)
x2 = layers.Dense(1024,activation = "relu")(x2)
x2 = layers.Dropout(0.3)(x)
x2 = layers.Dense(1024,activation = "relu")(x2)
x2 = layers.Dropout(0.3)(x)
x2 = layers.Dense(512,activation = "relu")(x2)
x2 = layers.Dense(256,activation = "relu")(x2)
x2 = layers.Dense(128,activation = "relu")(x2)
outputs2 = layers.Dense(10, activation="softmax")(x2)
model2 = keras.Model(inputs=inputs2, outputs=outputs2)

model2.summary()
#Compile
model2.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        start_from_epoch=5)
]
#Fitting
history2 = model2.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)


inputs3 = keras.Input(shape=(32, 32, 3))

x3 = layers.Conv2D(filters = 128, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(inputs3)
x3 = layers.MaxPooling2D(pool_size=2)(x)
x3 = layers.Dropout(0.3)(x)

x3 = layers.Conv2D(filters=256, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-2))(x3)
x3 = layers.MaxPooling2D(pool_size=2)(x3)
x3 = layers.Dropout(0.3)(x)

x3 = x3 = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x3)
x3 = x3 = layers.Conv2D(filters = 512, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x3)
x3 = x3 = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-2))(x3)
x3 = layers.MaxPooling2D(pool_size=2)(x3)
x3 = layers.Dropout(0.3)(x3)

x3 = layers.Flatten()(x3)
x3 = layers.Dense(2048,activation = "relu")(x3)
x3 = layers.Dropout(0.5)(x3)
x3 = layers.Dense(1024,activation = "relu")(x3)
x3 = layers.Dropout(0.5)(x3)
x3 = layers.Dense(1024,activation = "relu")(x3)
x3 = layers.Dropout(0.5)(x3)
x3 = layers.Dense(512,activation = "relu")(x2)
x3 = layers.Dropout(0.5)(x3)
x3 = layers.Dense(256,activation = "relu")(x2)
x3 = layers.Dropout(0.3)(x3)
x3 = layers.Dense(128,activation = "relu")(x2)
outputs3 = layers.Dense(10, activation="softmax")(x3)
model3 = keras.Model(inputs=inputs3, outputs=outputs3)

model3.summary()
#Compile
model3.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        
        patience=5,
        start_from_epoch=5)
]
#Fitting
history3 = model3.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=64,callbacks = callbacks)


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)

x = layers.MaxPooling2D(pool_size=2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.05)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.05)(x)

x = layers.Flatten()(x)
x = layers.Dense(500,activation = "relu")(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_11 = keras.Model(inputs=inputs, outputs=outputs)

model_11.summary()
#Compile
model_11.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="cnn_cats_dogs_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
#Fitting
history_11 = model_11.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=128,callbacks = callbacks)


'o'


loss, acc = model_11.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")
plot(history_11.history['loss'], history_11.history['val_loss'], 'loss')
plot(history_11.history['categorical_accuracy'], history_11.history['val_categorical_accuracy'], 'categorical_accuracy')


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.1)(x)

x = layers.Flatten()(x)
x = layers.Dense(512,activation = "relu")(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_12 = keras.Model(inputs=inputs, outputs=outputs)

model_12.summary()
#Compile
model_12.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        
        patience=5,
        start_from_epoch=5)
]
#Fitting
history_12 = model_12.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=256,callbacks = callbacks)


loss, acc = model_12.evaluate(x_test, y_test)
print(f"Test accuracy: {acc:.3f}")
plot(history_12.history['loss'], history_12.history['val_loss'], 'loss')
plot(history_12.history['categorical_accuracy'], history_12.history['val_categorical_accuracy'], 'categorical_accuracy')#


inputs = keras.Input(shape=(32, 32, 3))

x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(inputs)
x = layers.Conv2D(filters = 32, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.2)(x)

x = layers.Conv2D(filters=64, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.Conv2D(filters=128, kernel_size= (3,3) ,activation="relu",padding ="same",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.2)(x)

x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = x = layers.Conv2D(filters = 256, kernel_size = (3,3),padding = "same",activation = "relu",kernel_regularizer = regularizers.l2(1e-3))(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Dropout(0.2)(x)

x = layers.Flatten()(x)
x = layers.Dense(512,activation = "relu")(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(100,activation = "relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_13 = keras.Model(inputs=inputs, outputs=outputs)

model_13.summary()
#Compile
model_13.compile(optimizer="Adam",
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()])
#Callback
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        
        patience=10,
        start_from_epoch=10)
]
#Fitting
history_13 = model_13.fit(x_train, y_train,validation_split = 0.1 ,epochs=75, batch_size=512,callbacks = callbacks)



