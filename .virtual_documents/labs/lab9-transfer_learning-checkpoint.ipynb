


# Ejemplo de instalación de tensorflow 2.0
#%tensorflow_version 2.x
# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL
import tensorflow as tf

# Hacemos los imports que sean necesarios
import numpy as np
import matplotlib.pyplot as plt





import tensorflow_datasets as tfds

(ds_train, ds_test), ds_info = tfds.load(
    'stanford_dogs', split=['train', 'test'], with_info=True, as_supervised=True
)
NUM_CLASSES = ds_info.features['label'].num_classes





# ds_train es un iterable con una tupla (imagen, clase). Vamos a ver la resolución de la imagen
for i, element in enumerate(ds_train.take(5)):
    print(element[0].shape)





## TODO: completa el código en las partes donde hay un None 
# hecho
tamanoDeseado = (224,224)
ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, tamanoDeseado) , label))
ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, tamanoDeseado) , label))





def format_label(label):
 
    string_label = label_info.int2str(label)
    return string_label.split("-")[1]

label_info = ds_info.features["label"]

for i, (image, label) in enumerate(ds_train.take(9)):
    plt.subplot(3, 3, i + 1)
    plt.imshow(image.numpy().astype("uint8"))
    plt.title("{}".format(format_label(label)))
    plt.axis("off")





## TODO: aplica la función de preprocesado a los conjuntos del dataset
#hecho
ds_train = ds_train.map(lambda image, label : (tf.keras.applications.densenet.preprocess_input(image), label))
ds_test = ds_test.map(lambda image, label : (tf.keras.applications.densenet.preprocess_input(image), label))





## TODO: convierte las etiquetas a tipo one hot.
#hecho
ds_train = ds_train.map(lambda image, label: (image, tf.one_hot(label, NUM_CLASSES)))
ds_test = ds_test.map(lambda image, label: (image, tf.one_hot(label, NUM_CLASSES)))





batch_size = 128
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size) # En test no es necesario aleatorizar








#from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

## TODO: crea el modelo de data augmentation, rellenando los huecos marcados con None
#hecho
img_augmentation = Sequential(
    [
        tf.keras.layers.RandomRotation(factor = .15), # rotaciones
        tf.keras.layers.RandomTranslation(height_factor = .1, width_factor = .1), # traslaciones
        tf.keras.layers.RandomFlip(), # giros
        tf.keras.layers.RandomContrast(factor = .1),  # contraste
    ],
    name="img_augmentation",
)





from tensorflow.keras.applications import DenseNet121

# Cargamos el modelo. 
densenet = DenseNet121(
    weights='imagenet', # El parámetro weights le dice que cargue los datos de la red ya entrenada sobre el dataset ImageNet
    include_top=False, # El parámetro include_top le dice que cargue la última capa con el clasificador
    input_shape=(224, 224, 3), # Este parámetro será necesario para hacer transfer learning, 
                      # pues necesitaremos especificar el tamaño de los datos de entrada
)

# Comprobamos el tamaño de entrada de la red
print('Input shape :', densenet.input_shape)

# Comprobamos el tamaño de salida de la red
print('Output shape :', densenet.output_shape)







## TODO: crea el clasificador, rellenando los huecos marcados por None
#hecho
classifier = Sequential(
    [
        tf.keras.layers.GlobalAveragePooling2D(), # GlobalAveragePooling2D
        tf.keras.layers.BatchNormalization(), # BatchNormalization
        tf.keras.layers.Dropout(rate = .2), # Dropout con probabilidad 0.2
        tf.keras.layers.Dense(NUM_CLASSES, activation = 'softmax') # Dense con salida el número de clases del dataset
                                             # Softmax activation
    ]
)





## TODO: Crea el modelo final combinando los 3 modelos creados anteriormente

input = tf.keras.Input(shape = (224, 224, 3))  # capa de input, teniendo en cuenta el tamaño de las imágenes 

input_augmented = img_augmentation(inputs=input)  # aplica el modelo de data augmentation a la capa de entrada

densenet.trainable = False
densenet_output = densenet(inputs=input_augmented)  # aplica el modelo densenet

output = classifier(inputs=densenet_output)  # aplica el clasificador

model = tf.keras.Model(inputs=input_augmented, outputs=output, name="DenseNet")  # crea el modelo final, indicando la entrada y la salida
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)  # define el optimizador. Usaremos un Adam
model.compile(
    optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.Precision()]  # Compila el modelo, usando el optimizador creado, la función de pérdida para clasificación multiclase y la métrica de precisión
)





def plot_hist(hist):
    plt.plot(hist.history["precision_2"])
    # plt.plot(hist.history["val_accuracy"])
    plt.title("train precision")
    plt.ylabel("precision")
    plt.xlabel("epoch")
    # plt.legend(["train", "validation"], loc="upper left")
    plt.show()


hist = model.fit(
    ds_train_batch, epochs=3, verbose=1  # se podría poner validation_data=ds_test_batch, y así ver en cada iteración como va, pero en este caso consume mucho tiempo
)


print('TEST ACCURACY : ', model.evaluate(ds_test_batch)[-1])

plot_hist(hist)


hist.history





## TODO: completa aquellos puntos marcados con None
batch_size = 32 # Tenemos que bajar el tamaño anterior para no saturar la memoria del sistema
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size)

def unfreeze_model(model):
    # Descongelamos las últimas 20 capas while, dejando BatchNormalization sin entrenar
    for layer in model.layers[-20:]:
        if not isinstance(layer, tf.keras.layers.BatchNormalization): # Comprueba que la capa no sea de tipo BatchNormalization
            layer.trainable = True                        # Marca la capa como entrenable

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)                    # Usa Adam con una tasa de aprendizaje de 1e-5
    
    model.compile(
    optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.Precision()]  # define el compilador, la función de coste y la métrica
    )

unfreeze_model(model)                                    # Llama a la función que has creado sobre nuestro modelo

#epochs = 8  
hist = model.fit(
    ds_train_batch, epochs=3, verbose=1  # Llama a la función de entrenamiento sobre el modelo
)




plot_hist(hist)

print('TEST ACCURACY : ', model.evaluate(ds_test_batch)[-1])














# imports
# -------
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.applications import MobileNetV3Small
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt


### datos
# -----
(ds_train, ds_test), ds_info = tfds.load(
    'cifar10', split=['train', 'test'], with_info=True, as_supervised=True
)
NUM_CLASSES = ds_info.features['label'].num_classes

# preprocesado segun el modelo importado (no se puede decidir)
ds_train = ds_train.map(lambda image, label : (tf.keras.applications.mobilenet_v3.preprocess_input(image), label))
ds_test = ds_test.map(lambda image, label : (tf.keras.applications.mobilenet_v3.preprocess_input(image), label))

# one hot encoding
ds_train = ds_train.map(lambda image, label: (image, tf.one_hot(label, NUM_CLASSES)))
ds_test = ds_test.map(lambda image, label: (image, tf.one_hot(label, NUM_CLASSES)))

# batches
batch_size = 50
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size) # En test no es necesario aleatorizar


# partes del modelo
# -----------------

# entrada de la red, definimos el tamaño esperado
tamanoEntrada = (32, 32, 3)
# input
input = tf.keras.Input(shape = tamanoEntrada)  # capa de input, tamaño especificado



# primera accion del modelo es aumentar los datos, definimos eso 
img_augmentation = Sequential(
    [
        tf.keras.layers.RandomRotation(factor = .3), # rotaciones
        tf.keras.layers.RandomTranslation(height_factor = .2, width_factor = .2), # traslaciones
        tf.keras.layers.RandomFlip(), # giros
        tf.keras.layers.RandomContrast(factor = .2),  # contraste
    ],
    name="img_augmentation",
)


input = img_augmentation(input)

# modelo preentrenado, mobileNet, definimos los pesos y el tamaño
MobileNet = MobileNetV3Small(
    #input_shape = tamanoEntrada,
    input_tensor = input,
    include_top=False,
    weights="imagenet"
    #weights = "/Users/pepe/.keras/models/weights_mobilenet_v3_small_224_1.0_float.h5"
)
MobileNet.trainable = False

# ultimas capas, clasificador a nuestros datos particulares (cifar10)
classifier = Sequential(
    [
        tf.keras.layers.GlobalAveragePooling2D(), # GlobalAveragePooling2D
        tf.keras.layers.BatchNormalization(), # BatchNormalization
        tf.keras.layers.Dropout(rate = .05), # Dropout con probabilidad 0.2
        tf.keras.layers.Dense(NUM_CLASSES*8, activation = 'relu'),
        tf.keras.layers.Dropout(rate = .05),
        tf.keras.layers.Dense(NUM_CLASSES*4, activation = 'relu'),
        tf.keras.layers.Dropout(rate = .05),
        tf.keras.layers.Dense(NUM_CLASSES*2, activation = 'relu'),
        tf.keras.layers.Dropout(rate = .05),
        tf.keras.layers.Dense(NUM_CLASSES, activation = 'softmax') # Dense con salida el número de clases del dataset
                                             # Softmax activation
    ]
)


# ensamblaje
# ----------

# input
# (( hecho arriba ))
# input = tf.keras.Input(shape = tamanoEntrada)  # capa de input, tamaño especificado
# input = img_augmentation(input)

# output
output = MobileNet(inputs = input)
output = classifier(inputs = output)

# modelo
modelo = Model(inputs = input, outputs = output, name = "MobileNetV3Small")
modelo.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.CategoricalCrossentropy(), 
    metrics=[tf.keras.metrics.Precision()]
)





# entrenamiento
# -------------

def plot_hist(hist):
    plt.plot(hist.history["precision"])
    plt.plot(hist.history["val_precision"])
    plt.title("train precision")
    plt.ylabel("precision")
    plt.xlabel("epoch")
    # plt.legend(["train", "validation"], loc="upper left")
    plt.show()



hist = modelo.fit(
    ds_train_batch, epochs=500, verbose=1,
    validation_data=ds_test_batch,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)]
)


plot_hist(hist)

print('TEST ACCURACY : ', modelo.evaluate(ds_test_batch)[-1])


# fine tunin
# ----------

def unfreeze_model(model):
    # Descongelamos las últimas 20 capas while, dejando BatchNormalization sin entrenar
    for layer in model.layers[-20:]:
        if not isinstance(layer, tf.keras.layers.BatchNormalization): # Comprueba que la capa no sea de tipo BatchNormalization
            layer.trainable = True  
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)                    # Usa Adam con una tasa de aprendizaje de 1e-5
    
    modelo.compile(
    optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.Precision()]  # define el compilador, la función de coste y la métrica
    )



batch_size = 32 # Tenemos que bajar el tamaño anterior para no saturar la memoria del sistema
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size)

unfreeze_model(modelo)                                    # Llama a la función que has creado sobre nuestro modelo

#epochs = 8  
hist = modelo.fit(
    ds_train_batch, epochs=100, verbose=1,  # Llama a la función de entrenamiento sobre el modelo
    validation_data=ds_test_batch,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)]
)





