


# Ejemplo de instalación de tensorflow 2.0
#%tensorflow_version 2.x
# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL
import tensorflow as tf

# Hacemos los imports que sean necesarios
import numpy as np
import matplotlib.pyplot as plt





import tensorflow_datasets as tfds

(ds_train, ds_test), ds_info = tfds.load(
    'stanford_dogs', split=['train', 'test'], with_info=True, as_supervised=True
)
NUM_CLASSES = ds_info.features['label'].num_classes





# ds_train es un iterable con una tupla (imagen, clase). Vamos a ver la resolución de la imagen
for i, element in enumerate(ds_train.take(5)):
    print(element[0].shape)





## TODO: completa el código en las partes donde hay un None
ds_train = ds_train.map(lambda image, label: (None, label))
ds_test = None





def format_label(label):
    string_label = label_info.int2str(label)
    return string_label.split("-")[1]

label_info = ds_info.features["label"]
for i, (image, label) in enumerate(ds_train.take(9)):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(image.numpy().astype("uint8"))
    plt.title("{}".format(format_label(label)))
    plt.axis("off")





## TODO: aplica la función de preprocesado a los conjuntos del dataset
None
None





## TODO: convierte las etiquetas a tipo one hot.
None
None





batch_size = 128
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size) # En test no es necesario aleatorizar








from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

## TODO: crea el modelo de data augmentation, rellenando los huecos marcados con None
img_augmentation = Sequential(
    [
        None, # rotaciones
        None, # traslaciones
        None, # giros
        None  # contraste
    ],
    name="img_augmentation",
)





from tensorflow.keras.applications import DenseNet121

# Cargamos el modelo. 
densenet = DenseNet121(
    weights='imagenet', # El parámetro weights le dice que cargue los datos de la red ya entrenada sobre el dataset ImageNet
    include_top=False, # El parámetro include_top le dice que cargue la última capa con el clasificador
    input_shape=(224, 224, 3), # Este parámetro será necesario para hacer transfer learning, 
                      # pues necesitaremos especificar el tamaño de los datos de entrada
)

# Comprobamos el tamaño de entrada de la red
print('Input shape :', densenet.input_shape)

# Comprobamos el tamaño de salida de la red
print('Output shape :', densenet.output_shape)






from tensorflow.keras import layers

## TODO: crea el clasificador, rellenando los huecos marcados por None
classifier = Sequential(
    [
        None, # GlobalAveragePooling2D
        None, # BatchNormalization
        None, # Dropout con probabilidad 0.2
        None, # Dense con salida el número de clases del dataset
        None, # Softmax activation
    ]
)





## TODO: Crea el modelo final combinando los 3 modelos creados anteriormente

input = None  # capa de input, teniendo en cuenta el tamaño de las imágenes 

input_augmented = None  # aplica el modelo de data augmentation a la capa de entrada

densenet.trainable = False
densenet_output = None  # aplica el modelo densenet

output = None  # aplica el clasificador

model = tf.keras.Model(None, None, name="DenseNet")  # crea el modelo final, indicando la entrada y la salida
optimizer = None  # define el optimizador. Usaremos un Adam
model.compile(
    optimizer=None, loss=None, metrics=[None]  # Compila el modelo, usando el optimizador creado, la función de pérdida para clasificación multiclase y la métrica de precisión
)





hist = model.fit(
    ds_train_batch, epochs=10, verbose=1  # se podría poner validation_data=ds_test_batch, y así ver en cada iteración como va, pero en este caso consume mucho tiempo
)


def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    # plt.plot(hist.history["val_accuracy"])
    plt.title("train accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    # plt.legend(["train", "validation"], loc="upper left")
    plt.show()

print('TEST ACCURACY : ', model.evaluate(ds_test_batch)[-1])

plot_hist(hist)





## TODO: completa aquellos puntos marcados con None
batch_size = 32 # Tenemos que bajar el tamaño anterior para no saturar la memoria del sistema
ds_train_batch = ds_train.cache().shuffle(batch_size*5).batch(batch_size)
ds_test_batch = ds_test.batch(batch_size)

def unfreeze_model(model):
    # Descongelamos las últimas 20 capas while, dejando BatchNormalization sin entrenar
    for layer in model.layers[-20:]:
        if not isinstance(layer, None): # Comprueba que la capa no sea de tipo BatchNormalization
            None                        # Marca la capa como entrenable

    optimizer = None                    # Usa Adam con una tasa de aprendizaje de 1e-5
    model.compile(
        None, None, None                # define el compilador, la función de coste y la métrica
    )

None                                    # Llama a la función que has creado sobre nuestro modelo

epochs = 8  
hist = None  # Llama a la función de entrenamiento sobre el modelo

plot_hist(hist)

print('TEST ACCURACY : ', model.evaluate(ds_test_batch)[-1])











# TODO: escribe el código para el trabajo extra sin ayuda. Usa todos los bloques de código que quieras.
